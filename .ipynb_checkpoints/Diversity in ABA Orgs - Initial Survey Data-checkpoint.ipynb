{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a9ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the necessary packages for data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Specifying preferences for the display of data\n",
    "pd.options.display.max_columns=None #Show all columns\n",
    "pd.options.display.max_rows=None #Show all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9b6f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 247\n"
     ]
    }
   ],
   "source": [
    "#Reading in the raw data\n",
    "raw_data = pd.read_excel('Div in ABA survey data 9-6-23.xlsx', sheet_name=\"Sheet1\")\n",
    "\n",
    "row_count = len(raw_data)\n",
    "print(\"Number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f2a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 245\n"
     ]
    }
   ],
   "source": [
    "# Dropping columns that won't be included in analyses\n",
    "dropped_data = raw_data.drop(['Clinical_Influences', 'Finances_Influences', 'HR_Influences', \\\n",
    "                                  'Payor_Influences', 'Position_Title', 'Open_Comments' ], axis=1)\n",
    "\n",
    "# Dropping rows that have more than 10 blank cells\n",
    "trimmed_data = dropped_data.dropna(thresh=10) #drop rows if \"na\" in 10+ cells\n",
    "\n",
    "row_count = len(trimmed_data)\n",
    "print(\"Number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e9ed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 245\n"
     ]
    }
   ],
   "source": [
    "#Turning raw text into binary features via one-hot encoding for the service setting column\n",
    "#Specifying the new columns\n",
    "clinic_outpatient=[]\n",
    "community=[]\n",
    "home=[]\n",
    "hospital_inpatient=[]\n",
    "residential_group_home=[]\n",
    "school=[]\n",
    "vocational_program=[]\n",
    "other_setting=[]\n",
    "setting_unknown=[]\n",
    "\n",
    "#Specifying the text to include and exclude in each new column\n",
    "for i in trimmed_data[\"Service_Settings\"]:\n",
    "    i = str (i).lower() #Turning all text to lowercase\n",
    "    if \"clinic\" in i: \n",
    "        clinic_outpatient.append(1)\n",
    "    else:\n",
    "        clinic_outpatient.append(0)\n",
    "    if (\"community\" in i) and (\"retired from community\" not in i):\n",
    "        community.append(1)\n",
    "    else:\n",
    "        community.append(0)\n",
    "    if (\"home\" in i) and (\"residential/group home\" not in i) and (\"nursing home\" not in i)\\\n",
    "    and (\"retired from community, home and residential\" not in i):\n",
    "        home.append(1)\n",
    "    else:\n",
    "        home.append(0)\n",
    "    if \"hospital\" in i:\n",
    "        hospital_inpatient.append(1)\n",
    "    else:\n",
    "        hospital_inpatient.append(0)\n",
    "    if \"residential/group home\" in i:\n",
    "        residential_group_home.append(1)\n",
    "    else:\n",
    "        residential_group_home.append(0)\n",
    "    if (\"school\" in i) or (\"distric\" in i):\n",
    "        school.append(1)\n",
    "    else:\n",
    "        school.append(0)\n",
    "    if \"vocational program\" in i:\n",
    "        vocational_program.append(1)\n",
    "    else:\n",
    "        vocational_program.append(0)\n",
    "    if (\"telehealth\" in i) or (\"remote\" in i) or (\"daycare center\" in i) or (\"day cares\" in i) \\\n",
    "    or (\"day habs\" in i) or (\"adult day program\" in i) or (\"assisted living facilities\" in i) \\\n",
    "    or (\"memory care facilities\" in i) or (\"memory care facilities\" in i) or (\"nursing homes\" in i) \\\n",
    "    or (\"state supported living center\" in i) or (\"college\" in i) or (\"state government\" in i) \\\n",
    "    or (\"training crisis management\" in i) or (\"retired\" in i):\n",
    "        other_setting.append(1)\n",
    "    else:\n",
    "        other_setting.append(0)\n",
    "    if \"nan\" in i:\n",
    "        setting_unknown.append(1)\n",
    "    else:\n",
    "        setting_unknown.append(0)\n",
    "\n",
    "#Converting the list to a single dataframe\n",
    "service_settings_df = {\n",
    "    'clinic_outpatient' : clinic_outpatient,\n",
    "    'community' : community,\n",
    "    'home' : home,\n",
    "    'hospital_inpatient' : hospital_inpatient,\n",
    "    'residential_group_home' : residential_group_home,\n",
    "    'school' : school,\n",
    "    'vocational_program' : vocational_program,\n",
    "    'other_setting' : other_setting,\n",
    "    'setting_unknown' : setting_unknown\n",
    "}\n",
    "\n",
    "service_settings_df = pd.DataFrame(service_settings_df)\n",
    "\n",
    "#Adding an index column to allow for a clear merge with other dataframes later\n",
    "service_settings_df['IDX'] = service_settings_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3f92311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Service Setting  Count of 1s  Percentage of 1s\n",
      "clinic_outpatient            clinic_outpatient          164         66.938776\n",
      "school                                  school          111         45.306122\n",
      "home                                      home          110         44.897959\n",
      "community                            community          103         42.040816\n",
      "residential_group_home  residential_group_home           38         15.510204\n",
      "vocational_program          vocational_program           18          7.346939\n",
      "other_setting                    other_setting           12          4.897959\n",
      "hospital_inpatient          hospital_inpatient            3          1.224490\n",
      "setting_unknown                setting_unknown            1          0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(service_settings_df):\n",
    "    count_of_ones = service_settings_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / service_settings_df.count()) * 100  # Percentage of 1s\n",
    "    settings_desc_df = pd.DataFrame({\n",
    "        'Service Setting' : service_settings_df.columns,\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return settings_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "settings_desc_df = binary_count_and_percentage(service_settings_df)\n",
    "\n",
    "#Drop IDX\n",
    "settings_desc_df = settings_desc_df.drop('IDX')\n",
    "\n",
    "# Display the result\n",
    "print(settings_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a832b268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 245\n"
     ]
    }
   ],
   "source": [
    "#Ordinal encoding to transfer the org_size column text categories into ordinal values.\n",
    "org_size=[]\n",
    "\n",
    "#Specifying the ordinal values\n",
    "size_mapping = {\n",
    "    \"1-10 employees\": 1,\n",
    "    \"11-50 employees\": 2,\n",
    "    \"51-200 employees\": 3,\n",
    "    \"201-500 employees\": 4,\n",
    "    \"501-1,000 employees\": 5,\n",
    "    \"1,001-5,000 employees\": 6,\n",
    "    \"5,001-10,000 employees\" : 7,\n",
    "    \"10,000+ employees\" : 8\n",
    "}\n",
    "\n",
    "#Loop through to encode the above\n",
    "for i in trimmed_data[\"N_Employees\"]:\n",
    "    org_size.append(size_mapping.get(i, -1))\n",
    "\n",
    "#Creating a new dataframe with the ordinal values\n",
    "org_size_df = {\n",
    "    'org_size' : org_size,\n",
    "\n",
    "}\n",
    "    \n",
    "org_size_df = pd.DataFrame(org_size_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "org_size_df['IDX'] = org_size_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5662e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "2     80   32.653061\n",
      "3     53   21.632653\n",
      "4     36   14.693878\n",
      "1     27   11.020408\n",
      "5     19    7.755102\n",
      "6     18    7.346939\n",
      "7      7    2.857143\n",
      "8      5    2.040816\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate both count and percentage occurrence of each number\n",
    "def count_and_percentage_occurrence(org_size):\n",
    "    count = org_size.value_counts()  # Get counts\n",
    "    percentage = (count / len(org_size)) * 100  # Calculate percentages\n",
    "    org_size_desc_df = pd.DataFrame({'Count': count, 'Percentage': percentage})\n",
    "    return org_size_desc_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Call the function\n",
    "org_size_desc_df = count_and_percentage_occurrence(org_size_df['org_size'])\n",
    "\n",
    "# Display the count and percentage occurrence for each number\n",
    "print(org_size_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ade44c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Converting text to binary values via one-hot encoding for the geographic region column\n",
    "#Specifying the new columns\n",
    "midwest_us=[]\n",
    "south_us=[]\n",
    "west_us=[]\n",
    "northeast_us=[]\n",
    "other_geo=[]\n",
    "geo_unknown=[]\n",
    "\n",
    "#Specifying the text to include and exclude for each new column\n",
    "for i in trimmed_data[\"Geo_Region\"]:\n",
    "    i = str (i).lower()\n",
    "    if \"nd, sd, ne, ks\" in i: \n",
    "        midwest_us.append(1)\n",
    "    else:\n",
    "        midwest_us.append(0)\n",
    "    if \"de, md, wv, va, ky\" in i:\n",
    "        south_us.append(1)\n",
    "    else:\n",
    "        south_us.append(0)\n",
    "    if \"wa, or, id, mt, wy\" in i:\n",
    "        west_us.append(1)\n",
    "    else:\n",
    "        west_us.append(0)\n",
    "    if \"me, vt, nh, ma\" in i:\n",
    "        northeast_us.append(1)\n",
    "    else:\n",
    "        northeast_us.append(0)\n",
    "    if (\"british columbia\" in i) or (\"canada\" in i) or (\"ontario\" in i) or (\"internationally\" in i) \\\n",
    "    or (\"middle east\" in i) or (\"saudi arabia\" in i) or (\"riyadh\" in i) or (\"brazil\" in i) or (\"usvi\" in i) \\\n",
    "    or (\"brazil\" in i):\n",
    "        other_geo.append(1)\n",
    "    else:\n",
    "        other_geo.append(0)\n",
    "    if \"nan\" in i:\n",
    "        geo_unknown.append(1)\n",
    "    else:\n",
    "        geo_unknown.append(0)\n",
    "\n",
    "#Creating the new dataframe with the binary data\n",
    "geo_df = {\n",
    "    'midwest_us' : midwest_us,\n",
    "    'south_us' : south_us,\n",
    "    'west_us' : west_us,\n",
    "    'northeast_us' : northeast_us,\n",
    "    'other_geo' : other_geo,\n",
    "    'geo_unknown' : geo_unknown\n",
    "\n",
    "}\n",
    "\n",
    "geo_df = pd.DataFrame(geo_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "geo_df['IDX'] = geo_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5daff227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Geo Region  Count of 1s  Percentage of 1s\n",
      "south_us          south_us           96         39.183673\n",
      "midwest_us      midwest_us           64         26.122449\n",
      "northeast_us  northeast_us           64         26.122449\n",
      "west_us            west_us           63         25.714286\n",
      "other_geo        other_geo           12          4.897959\n",
      "geo_unknown    geo_unknown            1          0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(geo_df):\n",
    "    count_of_ones = geo_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / geo_df.count()) * 100  # Percentage of 1s\n",
    "    geo_desc_df = pd.DataFrame({\n",
    "        'Geo Region' : geo_df.columns,\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return geo_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "geo_desc_df = binary_count_and_percentage(geo_df)\n",
    "\n",
    "#drop the IDX\n",
    "geo_desc_df = geo_desc_df.drop ('IDX')\n",
    "\n",
    "# Display the result\n",
    "print(geo_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d40d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the ceo background column\n",
    "#Specifying the new columns\n",
    "ceo_behavior_analysis=[]\n",
    "ceo_business=[]\n",
    "ceo_education=[]\n",
    "ceo_finance=[]\n",
    "ceo_psychology=[]\n",
    "ceo_human_resources=[]\n",
    "ceo_organizational_behavior_management=[]\n",
    "ceo_other_allied=[]\n",
    "ceo_other_other=[]\n",
    "ceo_unknown=[]\n",
    "\n",
    "#Specifying what to include or exclude in the new columns\n",
    "for i in trimmed_data[\"CEO_Background\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"analysis\" in i) or (\"bcba\" in i): \n",
    "        ceo_behavior_analysis.append(1)\n",
    "    else:\n",
    "        ceo_behavior_analysis.append(0)\n",
    "    if \"business\" in i:\n",
    "        ceo_business.append(1)\n",
    "    else:\n",
    "        ceo_business.append(0)\n",
    "    if (\"education\" in i) or (\"m.ed.\" in i):\n",
    "        ceo_education.append(1)\n",
    "    else:\n",
    "        ceo_education.append(0)\n",
    "    if \"finance\" in i:\n",
    "        ceo_finance.append(1)\n",
    "    else:\n",
    "        ceo_finance.append(0)\n",
    "    if \"resources\" in i:\n",
    "        ceo_human_resources.append(1)\n",
    "    else:\n",
    "        ceo_human_resources.append(0)\n",
    "    if \"psychology\" in i:\n",
    "        ceo_psychology.append(1)\n",
    "    else:\n",
    "        ceo_psychology.append(0)\n",
    "    if \"organizational behavior\" in i:\n",
    "        ceo_organizational_behavior_management.append(1)\n",
    "    else:\n",
    "        ceo_organizational_behavior_management.append(0)\n",
    "    if (\"healthcare\" in i) or (\"health care\" in i) or (\"behavioral health\" in i) or (\"public health\" in i) \\\n",
    "    or (\"speech\" in i) or (\"occupational therapy\" in i) or (\"social work\" in i) or (\"counseling\" in i) \\\n",
    "    or (\"md\" in i) or (\"physician\" in i) or (\"neurology\" in i): \n",
    "        ceo_other_allied.append(1)\n",
    "    else:\n",
    "        ceo_other_allied.append(0)\n",
    "    if (\"sociology\" in i) or (\"law\" in i) or (\"politics\" in i) or (\"political science\" in i) \\\n",
    "    or (\"public school\" in i) or (\"university\" in i) or (\"administration\" in i):\n",
    "        ceo_other_other.append(1)\n",
    "    else:\n",
    "        ceo_other_other.append(0)\n",
    "    if (\"nan\" in i) or (\"i don't know\" in i):\n",
    "        ceo_unknown.append(1)\n",
    "    else:\n",
    "        ceo_unknown.append(0)\n",
    "\n",
    "#Creating the new dataframe\n",
    "ceo_background_frame = {\n",
    "    'ceo_behavior_analysis' : ceo_behavior_analysis,\n",
    "    'ceo_business' : ceo_business,\n",
    "    'ceo_education' : ceo_education,\n",
    "    'ceo_finance' : ceo_finance,\n",
    "    'ceo_psychology' : ceo_psychology,\n",
    "    'ceo_human_resources' : ceo_human_resources,\n",
    "    'ceo_organizational_behavior_management' : ceo_organizational_behavior_management,\n",
    "    'ceo_other_allied' : ceo_other_allied,\n",
    "    'ceo_other_other' : ceo_other_other,\n",
    "    'ceo_unknown' : ceo_unknown\n",
    "}\n",
    "\n",
    "ceo_df = pd.DataFrame(ceo_background_frame)\n",
    "\n",
    "#Creating an index column for future use\n",
    "ceo_df['IDX'] = ceo_df.index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53f02fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Count of 1s  Percentage of 1s\n",
      "IDX                                           29890      12200.000000\n",
      "ceo_behavior_analysis                           130         53.061224\n",
      "ceo_business                                     81         33.061224\n",
      "ceo_psychology                                   46         18.775510\n",
      "ceo_education                                    42         17.142857\n",
      "ceo_other_allied                                 23          9.387755\n",
      "ceo_unknown                                      15          6.122449\n",
      "ceo_organizational_behavior_management           14          5.714286\n",
      "ceo_finance                                      13          5.306122\n",
      "ceo_other_other                                  10          4.081633\n",
      "ceo_human_resources                               4          1.632653\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(ceo_df):\n",
    "    count_of_ones = ceo_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / ceo_df.count()) * 100  # Percentage of 1s\n",
    "    ceo_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return ceo_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "ceo_desc_df = binary_count_and_percentage(ceo_df)\n",
    "\n",
    "# Display the result\n",
    "print(ceo_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "273f90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the current position column\n",
    "#Specifying the new columns\n",
    "board_member=[]\n",
    "case_manager=[]\n",
    "director=[]\n",
    "direct_service=[]\n",
    "executive_member=[]\n",
    "position_other=[]\n",
    "position_unknown=[]\n",
    "\n",
    "#Specifying what to include and exclude for each new column\n",
    "for i in trimmed_data[\"Position_Category\"]:\n",
    "    i = str (i).lower()\n",
    "    if \"board member\" in i: \n",
    "        board_member.append(1)\n",
    "    else:\n",
    "        board_member.append(0)\n",
    "    if (\"case manager\" in i) or (\"clinical;\" in i) or (\"clinical,\" in i) \\\n",
    "    or (\"clinician\" in i) or (\"practitioner\" in i) or (\"supervisor\" in i):\n",
    "        case_manager.append(1)\n",
    "    else:\n",
    "        case_manager.append(0)\n",
    "    if (\"department director\" in i) or (\"regional or clinic director\" in i) or (\"regional coordinator\" in i):\n",
    "        director.append(1)\n",
    "    else:\n",
    "        director.append(0)\n",
    "    if (\"direct service\" in i) or (\"rbt\" in i):\n",
    "        direct_service.append(1)\n",
    "    else:\n",
    "        direct_service.append(0)\n",
    "    if (\"executive member\" in i) or (\"chief clinical officer\" in i) or (\"avp\" in i) or (\"owner\" in i):\n",
    "        executive_member.append(1)\n",
    "    else:\n",
    "        executive_member.append(0)\n",
    "    if (\"bureaucrat\" in i) or (\"slt member\" in i) or (\"management\" in i) or (\"teacher\" in i) \\\n",
    "    or (\"consultant\" in i) or (\"research\" in i) or (\"trainer\" in i) or (\"admin\" in i):\n",
    "        position_other.append(1)\n",
    "    else:\n",
    "        position_other.append(0)\n",
    "    if \"nan\" in i:\n",
    "        position_unknown.append(1)\n",
    "    else:\n",
    "        position_unknown.append(0)\n",
    "\n",
    "#Creating the new dataframe with the binary values\n",
    "position_category_frame = {\n",
    "    'board_member' : board_member,\n",
    "    'case_manager' : case_manager,\n",
    "    'director' : director,\n",
    "    'direct_service' : direct_service,\n",
    "    'executive_member' : executive_member,\n",
    "    'position_other' : position_other,\n",
    "    'position_unknown' : position_unknown\n",
    "\n",
    "}\n",
    "\n",
    "position_category_df = pd.DataFrame(position_category_frame)\n",
    "\n",
    "#Adding an index column for future use\n",
    "position_category_df['IDX'] = position_category_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b6bdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Count of 1s  Percentage of 1s\n",
      "IDX                     29890      12200.000000\n",
      "director                  105         42.857143\n",
      "case_manager               83         33.877551\n",
      "executive_member           59         24.081633\n",
      "direct_service             20          8.163265\n",
      "position_other             13          5.306122\n",
      "position_unknown            6          2.448980\n",
      "board_member                4          1.632653\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(position_category_df):\n",
    "    count_of_ones = position_category_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / position_category_df.count()) * 100  # Percentage of 1s\n",
    "    position_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return position_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "position_desc_df = binary_count_and_percentage(position_category_df)\n",
    "\n",
    "# Display the result\n",
    "print(position_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36d8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal encoding for the position duration column to convert the categorical text to ordinal values.\n",
    "position_duration=[]\n",
    "\n",
    "#Specifying the values for encoding\n",
    "years_mapping = {\n",
    "    \"1 year or less\": 1,\n",
    "    \"1-5 years\" : 2,\n",
    "    \"2-5 years\": 2,\n",
    "    \"6-10 years\": 3,\n",
    "    \"11-15 years\": 4,\n",
    "    \"More than 15 years\": 5,\n",
    "\n",
    "}\n",
    "\n",
    "#Looping through the column to encode\n",
    "for i in trimmed_data[\"Position_Duration\"]:\n",
    "    position_duration.append(years_mapping.get(i, -1))\n",
    "\n",
    "#Creating a new dataframe for the ordinal values\n",
    "position_duration_df = {\n",
    "    'position_duration' : position_duration,\n",
    "\n",
    "}\n",
    "    \n",
    "position_duration_df = pd.DataFrame(position_duration_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "position_duration_df['IDX'] = position_duration_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89bec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Count  Percentage\n",
      " 2    130   53.061224\n",
      " 3     44   17.959184\n",
      " 1     42   17.142857\n",
      " 4     19    7.755102\n",
      " 5      9    3.673469\n",
      "-1      1    0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate both count and percentage occurrence of each number\n",
    "def count_and_percentage_occurrence(position_duration):\n",
    "    count = position_duration.value_counts()  # Get counts\n",
    "    percentage = (count / len(position_duration)) * 100  # Calculate percentages\n",
    "    pos_dur_desc_df = pd.DataFrame({'Count': count, 'Percentage': percentage})\n",
    "    return pos_dur_desc_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Call the function\n",
    "pos_dur_desc_df = count_and_percentage_occurrence(position_duration_df['position_duration'])\n",
    "\n",
    "# Display the count and percentage occurrence for each number\n",
    "print(pos_dur_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a62c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal encoding for the education/degree column\n",
    "degree_ordinal = []\n",
    "\n",
    "#Specifying the ordinal values\n",
    "degree_mapping = {\n",
    "    \"bachelor's degree\\xa0\": 1,\n",
    "    \"master's degree\\xa0\": 2,\n",
    "    \"doctoral degree\\xa0\": 3\n",
    "}\n",
    "\n",
    "#Looping through the education column for encoding\n",
    "for i in trimmed_data[\"Education\"]:\n",
    "    if pd.isna(i):  # Check for NaN values\n",
    "        degree_ordinal.append(0)  # Append the mapped value for NaN\n",
    "    else:\n",
    "        i = str(i).lower()\n",
    "        degree_ordinal.append(degree_mapping.get(i, -1))  # Default to -1 if not found\n",
    "\n",
    "#Creating a new dataframe with the ordinal values\n",
    "degree_ordinal_df = {\n",
    "    'degree_ordinal': degree_ordinal\n",
    "}\n",
    "\n",
    "degree_ordinal_df = pd.DataFrame(degree_ordinal_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "degree_ordinal_df['IDX'] = degree_ordinal_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390291ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Count  Percentage\n",
      " 2    169   68.979592\n",
      " 3     52   21.224490\n",
      " 1     14    5.714286\n",
      "-1      9    3.673469\n",
      " 0      1    0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate both count and percentage occurrence of each number\n",
    "def count_and_percentage_occurrence(degree_ordinal_df):\n",
    "    count = degree_ordinal_df.value_counts()  # Get counts\n",
    "    percentage = (count / len(degree_ordinal_df)) * 100  # Calculate percentages\n",
    "    degree_desc_df = pd.DataFrame({'Count': count, 'Percentage': percentage})\n",
    "    return degree_desc_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Call the function\n",
    "degree_desc_df = count_and_percentage_occurrence(degree_ordinal_df['degree_ordinal'])\n",
    "\n",
    "# Display the count and percentage occurrence for each number\n",
    "print(degree_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "012dc509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the education domain column\n",
    "#Specifying the new columns\n",
    "ed_behavior_analysis=[]\n",
    "ed_business=[]\n",
    "ed_education=[]\n",
    "ed_finance=[]\n",
    "ed_hr=[]\n",
    "ed_obm=[]\n",
    "ed_psychology=[]\n",
    "ed_social_work=[]\n",
    "ed_domain_other=[]\n",
    "ed_domain_unknown=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Training_Domain\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"behavior analysis\" in i) or (\"beh anal\" in i): \n",
    "        ed_behavior_analysis.append(1)\n",
    "    else:\n",
    "        ed_behavior_analysis.append(0)\n",
    "    if \"business\" in i:\n",
    "        ed_business.append(1)\n",
    "    else:\n",
    "        ed_business.append(0)\n",
    "    if \"education\" in i:\n",
    "        ed_education.append(1)\n",
    "    else:\n",
    "        ed_education.append(0)\n",
    "    if \"finance\" in i:\n",
    "        ed_finance.append(1)\n",
    "    else:\n",
    "        ed_finance.append(0)\n",
    "    if \"human resources\" in i:\n",
    "        ed_hr.append(1)\n",
    "    else:\n",
    "        ed_hr.append(0)\n",
    "    if \"organizational behavior management\" in i:\n",
    "        ed_obm.append(1)\n",
    "    else:\n",
    "        ed_obm.append(0)\n",
    "    if \"psychology\" in i:\n",
    "        ed_psychology.append(1)\n",
    "    else:\n",
    "        ed_psychology.append(0)\n",
    "    if \"social work\" in i:\n",
    "        ed_social_work.append(1)\n",
    "    else:\n",
    "        ed_social_work.append(0)\n",
    "    if (\"recreational therapy\" in i) or (\"health promotion\" in i) or (\"organizational leadership\" in i)\\\n",
    "    or (\"counseling\" in i) or (\"sociology\" in i) or (\"neuroscience\" in i) or (\"journalism\" in i)\\\n",
    "    or (\"political science\" in i) or (\"public service\" in i) or (\"philosophy and theology\" in i)\\\n",
    "    or (\"communication sciences and disorders\" in i):\n",
    "        ed_domain_other.append(1)\n",
    "    else:\n",
    "        ed_domain_other.append(0)\n",
    "    if \"nan\" in i:\n",
    "        ed_domain_unknown.append(1)\n",
    "    else:\n",
    "        ed_domain_unknown.append(0)\n",
    "\n",
    "#Creating a new dataframe with the binary values\n",
    "training_df = {\n",
    "    'ed_behavior_analysis' : ed_behavior_analysis,\n",
    "    'ed_business' : ed_business,\n",
    "    'ed_education' : ed_education,\n",
    "    'ed_finance' : ed_finance,\n",
    "    'ed_hr' : ed_hr,\n",
    "    'ed_obm' : ed_obm,\n",
    "    'ed_psychology' : ed_psychology,\n",
    "    'ed_social_work' : ed_social_work,\n",
    "    'ed_domain_other' : ed_domain_other,\n",
    "    'ed_domain_unknown' : ed_domain_unknown\n",
    "\n",
    "}\n",
    "\n",
    "training_df = pd.DataFrame(training_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "training_df['IDX'] = training_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eceaf657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Count of 1s  Percentage of 1s\n",
      "IDX                         29890      12200.000000\n",
      "ed_behavior_analysis          236         96.326531\n",
      "ed_psychology                 119         48.571429\n",
      "ed_education                   98         40.000000\n",
      "ed_obm                         47         19.183673\n",
      "ed_domain_other                16          6.530612\n",
      "ed_business                    15          6.122449\n",
      "ed_social_work                 10          4.081633\n",
      "ed_hr                           7          2.857143\n",
      "ed_finance                      6          2.448980\n",
      "ed_domain_unknown               6          2.448980\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(training_df):\n",
    "    count_of_ones = training_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / training_df.count()) * 100  # Percentage of 1s\n",
    "    train_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return train_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "train_desc_df = binary_count_and_percentage(training_df)\n",
    "\n",
    "# Display the result\n",
    "print(train_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ff07f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the certification column\n",
    "#Specifying the new columns\n",
    "bcba=[]\n",
    "cpa=[]\n",
    "slp=[]\n",
    "lba=[]\n",
    "licensed_psychologist=[]\n",
    "licensed_sw=[]\n",
    "other_rbt_bcaba=[]\n",
    "other_cert=[]\n",
    "other_cert_teaching=[]\n",
    "no_cert=[]\n",
    "cert_unknown=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Certifications\"]:\n",
    "    i = str (i).lower()\n",
    "    if \"board certified behavior analyst\" in i: \n",
    "        bcba.append(1)\n",
    "    else:\n",
    "        bcba.append(0)\n",
    "    if (\"certified public accountant\" in i) or (\"lpa\" in i):\n",
    "        cpa.append(1)\n",
    "    else:\n",
    "        cpa.append(0)\n",
    "    if \"ccc-slp\" in i:\n",
    "        slp.append(1)\n",
    "    else:\n",
    "        slp.append(0)\n",
    "    if \"licensed behavior analyst\" in i:\n",
    "        lba.append(1)\n",
    "    else:\n",
    "        lba.append(0)\n",
    "    if (\"licensed psychologist\" in i) or (\"limited license psychologist\" in i) or (\"school psychologist\" in i)\\\n",
    "    or (\"lssp\" in i):\n",
    "        licensed_psychologist.append(1)\n",
    "    else:\n",
    "        licensed_psychologist.append(0)\n",
    "    if \"licensed social worker\" in i:\n",
    "        licensed_sw.append(1)\n",
    "    else:\n",
    "        licensed_sw.append(0)\n",
    "    if (\"rbt\" in i) or (\"registered behavior technician\" in i) or (\"board certified assistant behavior analyst\" in i):\n",
    "        other_rbt_bcaba.append(1)\n",
    "    else:\n",
    "        other_rbt_bcaba.append(0)\n",
    "    if (\"certified special educator\" in i) or (\"certified teacher\" in i) or (\"prek-12 license\" in i) \\\n",
    "    or (\"teaching license\" in i) or (\"licensed educator\" in i) or (\"special education experienced educator\" in i) \\\n",
    "    or (\"special education teacher\" in i) or (\"special education teaching\" in i) or (\"licensed sped teacher\" in i) \\\n",
    "    or (\"licensed special educator\" in i) or (\"teacher certification\" in i) or (\"education specialist\" in i) or\\\n",
    "    (\"educational assistant\" in i) or (\"sp ed teacher\" in i):\n",
    "        other_cert_teaching.append(1)\n",
    "    else:\n",
    "        other_cert_teaching.append(0)\n",
    "    if (\"laba\" in i) or (\"certified ohio behavior analyst\" in i) or (\"licensed behavior specialist\" in i)\\\n",
    "    or (\"licensed recreational therapist\" in i) or (\"certified therapeutic recreation specialist\" in i)\\\n",
    "    or (\"certified autism specialist\" in i) or (\"qasp-s\" in i) or (\"certified scrum master\" in i) \\\n",
    "    or (\"iba\" in i) or (\"international behavior analyst\" in i) or (\"lmhc\" in i) or (\"lpc\" in i) \\\n",
    "    or (\"mental health counselor\" in i):\n",
    "        other_cert.append(1)\n",
    "    else:\n",
    "        other_cert.append(0)\n",
    "    if \"no certificate\" in i:\n",
    "        no_cert.append(1)\n",
    "    else:\n",
    "        no_cert.append(0)\n",
    "    if \"nan\" in i:\n",
    "        cert_unknown.append(1)\n",
    "    else:\n",
    "        cert_unknown.append(0)\n",
    "\n",
    "#Creating a new dataframe with the binary values\n",
    "certifications_df = {\n",
    "    'bcba' : bcba,\n",
    "    'cpa' : cpa,\n",
    "    'slp' : slp,\n",
    "    'lba' : lba,\n",
    "    'licensed_psychologist' : licensed_psychologist,\n",
    "    'licensed_sw' : licensed_sw,\n",
    "    'other_rbt_bcaba' : other_rbt_bcaba,\n",
    "    'other_cert_teaching' : other_cert_teaching,\n",
    "    'other_cert' : other_cert,\n",
    "    'no_cert' : no_cert,\n",
    "    'cert_unknown' : cert_unknown\n",
    "\n",
    "}\n",
    "\n",
    "certifications_df = pd.DataFrame(certifications_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "certifications_df['IDX'] = certifications_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c310c072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Count of 1s  Percentage of 1s\n",
      "IDX                          29890      12200.000000\n",
      "bcba                           214         87.346939\n",
      "lba                            116         47.346939\n",
      "licensed_psychologist           22          8.979592\n",
      "other_cert_teaching             18          7.346939\n",
      "other_rbt_bcaba                 15          6.122449\n",
      "other_cert                      15          6.122449\n",
      "cert_unknown                     8          3.265306\n",
      "slp                              2          0.816327\n",
      "licensed_sw                      2          0.816327\n",
      "cpa                              1          0.408163\n",
      "no_cert                          1          0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(certifications_df):\n",
    "    count_of_ones = certifications_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / certifications_df.count()) * 100  # Percentage of 1s\n",
    "    cert_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return cert_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "cert_desc_df = binary_count_and_percentage(certifications_df)\n",
    "\n",
    "# Display the result\n",
    "print(cert_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3095301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting categorical text to ordinal values for the age column\n",
    "age_ordinal=[]\n",
    "\n",
    "#Specifying the ordinal values\n",
    "age_mapping = {\n",
    "    \"18-24 years old\": 1,\n",
    "    \"25-34 years old\" : 2,\n",
    "    \"35-44 years old\": 3,\n",
    "    \"45-54 years old\": 4,\n",
    "    \"55-64 years old\": 5,\n",
    "    \"65 years or older\": 6,\n",
    "    \"prefer not to answer\" : 0\n",
    "\n",
    "}\n",
    "\n",
    "#Looping through the column for encoding\n",
    "for i in trimmed_data[\"Age\"]:\n",
    "    age_ordinal.append(age_mapping.get(i, -1))\n",
    "\n",
    "#Creating a new dataframe with the ordinal values\n",
    "age_ordinal_df = {\n",
    "    'age_ordinal' : age_ordinal\n",
    "\n",
    "}\n",
    "    \n",
    "age_ordinal_df = pd.DataFrame(age_ordinal_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "age_ordinal_df['IDX'] = age_ordinal_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aba9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Count  Percentage\n",
      " 3    100   40.816327\n",
      " 2     61   24.897959\n",
      " 4     46   18.775510\n",
      " 5     22    8.979592\n",
      " 6      9    3.673469\n",
      " 1      6    2.448980\n",
      "-1      1    0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate both count and percentage occurrence of each number\n",
    "def count_and_percentage_occurrence(age_ordinal_df):\n",
    "    count = age_ordinal_df.value_counts()  # Get counts\n",
    "    percentage = (count / len(age_ordinal_df)) * 100  # Calculate percentages\n",
    "    age_desc_df = pd.DataFrame({'Count': count, 'Percentage': percentage})\n",
    "    return age_desc_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Call the function\n",
    "age_desc_df = count_and_percentage_occurrence(age_ordinal_df['age_ordinal'])\n",
    "\n",
    "# Display the count and percentage occurrence for each number\n",
    "print(age_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9540fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the race column\n",
    "#Specifying the new columns\n",
    "american_indian_or_alaska_native=[]\n",
    "black_or_african_american=[]\n",
    "white=[]\n",
    "middle_eastern=[]\n",
    "arab=[]\n",
    "asian=[]\n",
    "hispanic_or_latinx=[]\n",
    "native_hawaiian_or_pacific_islander=[]\n",
    "race_mixed=[]\n",
    "race_prefer_not_to_answer=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Race\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"indian\" in i) or (\"native american\" in i): \n",
    "        american_indian_or_alaska_native.append(1)\n",
    "    else:\n",
    "        american_indian_or_alaska_native.append(0)\n",
    "    if \"black\" in i:\n",
    "        black_or_african_american.append(1)\n",
    "    else:\n",
    "        black_or_african_american.append(0)\n",
    "    if \"white\" in i:\n",
    "        white.append(1)\n",
    "    else:\n",
    "        white.append(0)\n",
    "    if \"middle eastern\" in i:\n",
    "        middle_eastern.append(1)\n",
    "    else:\n",
    "        middle_eastern.append(0)\n",
    "    if \"arab\" in i:\n",
    "        arab.append(1)\n",
    "    else:\n",
    "        arab.append(0)\n",
    "    if \"asian\" in i:\n",
    "        asian.append(1)\n",
    "    else:\n",
    "        asian.append(0)\n",
    "    if \"hispanic\" in i:\n",
    "        hispanic_or_latinx.append(1)\n",
    "    else:\n",
    "        hispanic_or_latinx.append(0)\n",
    "    if \"islander\" in i:\n",
    "        native_hawaiian_or_pacific_islander.append(1)\n",
    "    else:\n",
    "        native_hawaiian_or_pacific_islander.append(0)\n",
    "    if \"mixed\" in i: \n",
    "        race_mixed.append(1)\n",
    "    else:\n",
    "        race_mixed.append(0)\n",
    "    if \"prefer\" in i:\n",
    "        race_prefer_not_to_answer.append(1)\n",
    "    else:\n",
    "        race_prefer_not_to_answer.append(0)\n",
    "\n",
    "#Creating a new dataframe with the binary values       \n",
    "race_df = {\n",
    "    'american_indian_or_alaska_native' : american_indian_or_alaska_native,\n",
    "    'black_or_african_american' : black_or_african_american,\n",
    "    'white' : white,\n",
    "    'middle_eastern' : middle_eastern,\n",
    "    'arab' : arab,\n",
    "    'asian' : asian,\n",
    "    'hispanic_or_latinx' : hispanic_or_latinx,\n",
    "    'native_hawaiian_or_pacific_islander' : native_hawaiian_or_pacific_islander,\n",
    "    'race_mixed' : race_mixed,\n",
    "    'race_prefer_not_to_answer' : race_prefer_not_to_answer,\n",
    "}\n",
    "\n",
    "race_df = pd.DataFrame(race_df)\n",
    "\n",
    "#Adding an index column for future merging use\n",
    "race_df['IDX'] = race_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6ef6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Count of 1s  Percentage of 1s\n",
      "IDX                                        29890      12200.000000\n",
      "white                                        206         84.081633\n",
      "hispanic_or_latinx                            18          7.346939\n",
      "asian                                         16          6.530612\n",
      "black_or_african_american                     15          6.122449\n",
      "american_indian_or_alaska_native               3          1.224490\n",
      "middle_eastern                                 2          0.816327\n",
      "race_prefer_not_to_answer                      2          0.816327\n",
      "arab                                           1          0.408163\n",
      "native_hawaiian_or_pacific_islander            1          0.408163\n",
      "race_mixed                                     1          0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(race_df):\n",
    "    count_of_ones = race_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / race_df.count()) * 100  # Percentage of 1s\n",
    "    race_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return race_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "race_desc_df = binary_count_and_percentage(race_df)\n",
    "\n",
    "# Display the result\n",
    "print(race_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef86d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the gender column\n",
    "#Specifying the new columns\n",
    "nonbinary=[]\n",
    "cisgender_female=[]\n",
    "cisgender_male=[]\n",
    "female=[]\n",
    "transgender_female=[]\n",
    "transgender_male=[]\n",
    "gender_prefer_no_answer=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Gender\"]:\n",
    "    i = str (i).lower()\n",
    "    if \"nonbinary\" in i: \n",
    "        nonbinary.append(1)\n",
    "    else:\n",
    "        nonbinary.append(0)\n",
    "    if \"cisgender female\" in i:\n",
    "        cisgender_female.append(1)\n",
    "    else:\n",
    "        cisgender_female.append(0)\n",
    "    if \"cisgender male\" in i:\n",
    "        cisgender_male.append(1)\n",
    "    else:\n",
    "        cisgender_male.append(0)\n",
    "    if (\"female\" in i) and (\"cisgender female\" not in i) and (\"transgender female\" not in i):\n",
    "        female.append(1)\n",
    "    else:\n",
    "        female.append(0)\n",
    "    if \"transgender female\" in i:\n",
    "        transgender_female.append(1)\n",
    "    else:\n",
    "        transgender_female.append(0)\n",
    "    if \"transgender male\" in i:\n",
    "        transgender_male.append(1)\n",
    "    else:\n",
    "        transgender_male.append(0)\n",
    "    if \"prefer not to answer\" in i:\n",
    "        gender_prefer_no_answer.append(1)\n",
    "    else:\n",
    "        gender_prefer_no_answer.append(0)\n",
    "\n",
    "#Creating a new dataframe with the binary values\n",
    "gender_df = {\n",
    "    'nonbinary' : nonbinary,\n",
    "    'cisgender_female' : cisgender_female,\n",
    "    'cisgender_male' : cisgender_male,\n",
    "    'female' : female,\n",
    "    'transgender_female' : transgender_female,\n",
    "    'transgender_male' : transgender_male,\n",
    "    'gender_prefer_no_answer' : gender_prefer_no_answer,\n",
    "\n",
    "}\n",
    "\n",
    "gender_df = pd.DataFrame(gender_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "gender_df['IDX'] = gender_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9f004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Count of 1s  Percentage of 1s\n",
      "IDX                            29890      12200.000000\n",
      "cisgender_female                 200         81.632653\n",
      "cisgender_male                    29         11.836735\n",
      "female                             8          3.265306\n",
      "gender_prefer_no_answer            6          2.448980\n",
      "nonbinary                          1          0.408163\n",
      "transgender_female                 1          0.408163\n",
      "transgender_male                   0          0.000000\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(gender_df):\n",
    "    count_of_ones = gender_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / gender_df.count()) * 100  # Percentage of 1s\n",
    "    gender_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return gender_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "gender_desc_df = binary_count_and_percentage(gender_df)\n",
    "\n",
    "# Display the result\n",
    "print(gender_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "955a7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the sexual orientation column\n",
    "#Specifying the new columns\n",
    "asexual=[]\n",
    "bisexual=[]\n",
    "heterosexual=[]\n",
    "homosexual=[]\n",
    "pansexual=[]\n",
    "sex_prefer_no_answer=[]\n",
    "other_queer=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Sexual_Orientation\"]:\n",
    "    i = str (i).lower()\n",
    "    if \"asexual\" in i: \n",
    "        asexual.append(1)\n",
    "    else:\n",
    "        asexual.append(0)\n",
    "    if \"bisexual\" in i:\n",
    "        bisexual.append(1)\n",
    "    else:\n",
    "        bisexual.append(0)\n",
    "    if \"heterosexual\" in i:\n",
    "        heterosexual.append(1)\n",
    "    else:\n",
    "        heterosexual.append(0)\n",
    "    if \"homosexual\" in i:\n",
    "        homosexual.append(1)\n",
    "    else:\n",
    "        homosexual.append(0)\n",
    "    if \"pansexual\" in i:\n",
    "        pansexual.append(1)\n",
    "    else:\n",
    "        pansexual.append(0)\n",
    "    if \"prefer not to answer\" in i:\n",
    "        sex_prefer_no_answer.append(1)\n",
    "    else:\n",
    "        sex_prefer_no_answer.append(0)\n",
    "    if \"queer\" in i:\n",
    "        other_queer.append(1)\n",
    "    else:\n",
    "        other_queer.append(0)\n",
    "\n",
    "#Creating a new dataframe with the binary values\n",
    "sex_df = {\n",
    "    'asexual' : asexual,\n",
    "    'bisexual' : bisexual,\n",
    "    'heterosexual' : heterosexual,\n",
    "    'homosexual' : homosexual,\n",
    "    'pansexual' : pansexual,\n",
    "    'sex_prefer_no_answer' : sex_prefer_no_answer,\n",
    "    'other_queer' : other_queer,\n",
    "    \n",
    "}\n",
    "\n",
    "sex_df = pd.DataFrame(sex_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "sex_df['IDX'] = sex_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26c3462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Count of 1s  Percentage of 1s\n",
      "IDX                         29890      12200.000000\n",
      "heterosexual                  192         78.367347\n",
      "bisexual                       18          7.346939\n",
      "sex_prefer_no_answer           11          4.489796\n",
      "homosexual                      9          3.673469\n",
      "asexual                         7          2.857143\n",
      "pansexual                       7          2.857143\n",
      "other_queer                     1          0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(sex_df):\n",
    "    count_of_ones = sex_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / sex_df.count()) * 100  # Percentage of 1s\n",
    "    sex_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return sex_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "sex_desc_df = binary_count_and_percentage(sex_df)\n",
    "\n",
    "# Display the result\n",
    "print(sex_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58c63b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the religion column\n",
    "#Specifying the new columns\n",
    "buddhism=[]\n",
    "christianity=[]\n",
    "hinduism=[]\n",
    "indigenous_religion=[]\n",
    "islam=[]\n",
    "judaism=[]\n",
    "non_religious=[]\n",
    "religion_other=[]\n",
    "religion_prefer_no_answer=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Religious_Affiliation\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"agnost\" in i) or (\"atheis\" in i) or (\"spiritual\" in i): \n",
    "        religion_other.append(1)\n",
    "    else:\n",
    "        religion_other.append(0)\n",
    "\n",
    "    if \"buddhis\" in i:\n",
    "        buddhism.append(1)\n",
    "    else:\n",
    "        buddhism.append(0)\n",
    "    if (\"cathol\" in i) or (\"christianity\" in i):\n",
    "        christianity.append(1)\n",
    "    else:\n",
    "        christianity.append(0)\n",
    "    if \"hinduism\" in i:\n",
    "        hinduism.append(1)\n",
    "    else:\n",
    "        hinduism.append(0)\n",
    "    if \"indigenous\" in i:\n",
    "        indigenous_religion.append(1)\n",
    "    else:\n",
    "        indigenous_religion.append(0)\n",
    "    if \"islam\" in i:\n",
    "        islam.append(1)\n",
    "    else:\n",
    "        islam.append(0)\n",
    "    if \"judaism\" in i:\n",
    "        judaism.append(1)\n",
    "    else:\n",
    "        judaism.append(0)\n",
    "    if \"non\" in i:\n",
    "        non_religious.append(1)\n",
    "    else:\n",
    "        non_religious.append(0)\n",
    "    if \"pref\" in i:\n",
    "        religion_prefer_no_answer.append(1)\n",
    "    else:\n",
    "        religion_prefer_no_answer.append(0)\n",
    "\n",
    "#Creating a new dataframe with the binary values\n",
    "religious_affiliation_frame = {\n",
    "    'buddhism' : buddhism,\n",
    "    'christianity' : christianity,\n",
    "    'hinduism' : hinduism,\n",
    "    'indigenous_religion' : indigenous_religion,\n",
    "    'islam' : islam,\n",
    "    'judaism' : judaism,\n",
    "    'religion_other' : religion_other,\n",
    "    'non_religious' : non_religious,\n",
    "    'religion_prefer_no_answer' : religion_prefer_no_answer,\n",
    "}\n",
    "\n",
    "ra_df = pd.DataFrame(religious_affiliation_frame)\n",
    "\n",
    "#Creating an index column for future use\n",
    "ra_df['IDX'] = ra_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a89e3840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Count of 1s  Percentage of 1s\n",
      "IDX                              29890      12200.000000\n",
      "non_religious                      116         47.346939\n",
      "christianity                       101         41.224490\n",
      "religion_prefer_no_answer           13          5.306122\n",
      "judaism                             10          4.081633\n",
      "religion_other                       9          3.673469\n",
      "islam                                4          1.632653\n",
      "hinduism                             2          0.816327\n",
      "buddhism                             1          0.408163\n",
      "indigenous_religion                  1          0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(ra_df):\n",
    "    count_of_ones = ra_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / ra_df.count()) * 100  # Percentage of 1s\n",
    "    ra_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return ra_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "ra_desc_df = binary_count_and_percentage(ra_df)\n",
    "\n",
    "# Display the result\n",
    "print(ra_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "983c7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the veteran status column\n",
    "#Specifying the new columns\n",
    "protected_vet=[]\n",
    "not_protected_vet=[]\n",
    "other_family=[]\n",
    "vet_prefer_not_answer=[]\n",
    "vet_unknown=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Veteran_Status\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"i am a disabled veteran\" in i) or (\"i am a recently separated veteran\" in i) or \\\n",
    "    (\"i am an active duty wartime\" in i) or (\"i am an armed forces service medal veteran\" in i) \\\n",
    "    or (\"i am a va army national guard veteran\" in i) or (\"honorable discharge\" in i): \n",
    "        protected_vet.append(1)\n",
    "    else:\n",
    "        protected_vet.append(0)\n",
    "    if (\"i am not a protected veteran\" in i) or (\"military spouse\" in i) or (\"veteran spouse\" in i) \\\n",
    "    or (\"spouse of veteran\" in i) or (\"wife of a veteran\" in i) or (\"dependent of active duty\" in i) \\\n",
    "    or (\"veteran from a foreign arm\" in i) or (\"i am not a veteran\" in i) or (\"none\" in i) or \\\n",
    "    (\"not a veteran\" in i) or (\"not veteran\" in i):\n",
    "        not_protected_vet.append(1)\n",
    "    else:\n",
    "        not_protected_vet.append(0)\n",
    "    if (\"military spouse\" in i) or (\"veteran spouse\" in i) or (\"spouse of veteran\" in i) \\\n",
    "    or (\"wife of a veteran\" in i) or (\"dependent of active duty\" in i):\n",
    "        other_family.append(1)\n",
    "    else:\n",
    "        other_family.append(0)\n",
    "    if \"prefer not to answer\" in i:\n",
    "        vet_prefer_not_answer.append(1)\n",
    "    else:\n",
    "        vet_prefer_not_answer.append(0)\n",
    "    if \"nan\" in i:\n",
    "        vet_unknown.append(1)\n",
    "    else:\n",
    "        vet_unknown.append(0)\n",
    "\n",
    "#Creating a new dataframe with the binary values\n",
    "vet_df = {\n",
    "    'protected_vet' : protected_vet,\n",
    "    'not_protected_vet' : not_protected_vet,\n",
    "    'other_family' : other_family,\n",
    "    'vet_prefer_not_answer' : vet_prefer_not_answer,\n",
    "    'vet_unknown' : vet_unknown\n",
    "}\n",
    "\n",
    "vet_df = pd.DataFrame(vet_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "vet_df['IDX'] = vet_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "381b253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Count of 1s  Percentage of 1s\n",
      "IDX                          29890      12200.000000\n",
      "not_protected_vet              231         94.285714\n",
      "protected_vet                    7          2.857143\n",
      "other_family                     5          2.040816\n",
      "vet_unknown                      4          1.632653\n",
      "vet_prefer_not_answer            3          1.224490\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(vet_df):\n",
    "    count_of_ones = vet_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / vet_df.count()) * 100  # Percentage of 1s\n",
    "    vet_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return vet_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "vet_desc_df = binary_count_and_percentage(vet_df)\n",
    "\n",
    "# Display the result\n",
    "print(vet_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85c0a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to binary values for the disability column\n",
    "#Specifying the new columns\n",
    "no_disability=[]\n",
    "yes_disability=[]\n",
    "other_invisible_disability=[]\n",
    "dis_prefer_no_answer=[]\n",
    "disability_unknown=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Disability_Status\"]:\n",
    "    i = str (i).lower()\n",
    "    if \"i do not have a disability\" in i: \n",
    "        no_disability.append(1)\n",
    "    else:\n",
    "        no_disability.append(0)\n",
    "    if (\"i have a disability\" in i) or (\"invisible disability\" in i):\n",
    "        yes_disability.append(1)\n",
    "    else:\n",
    "        yes_disability.append(0)\n",
    "    if \"invisible disability\" in i:\n",
    "        other_invisible_disability.append(1)\n",
    "    else:\n",
    "        other_invisible_disability.append(0)\n",
    "    if \"prefer not to answer\" in i:\n",
    "        dis_prefer_no_answer.append(1)\n",
    "    else:\n",
    "        dis_prefer_no_answer.append(0)\n",
    "    if \"nan\" in i:\n",
    "        disability_unknown.append(1)\n",
    "    else:\n",
    "        disability_unknown.append(0)\n",
    "  \n",
    "#Creating a new dataframe with the binary values\n",
    "disability_df = {\n",
    "    'no_disability' : no_disability,\n",
    "    'yes_disability' : yes_disability,\n",
    "    'other_invisible_disability' : other_invisible_disability,\n",
    "    'dis_prefer_no_answer' : dis_prefer_no_answer,\n",
    "    'disability_unknown' : disability_unknown\n",
    "    \n",
    "}\n",
    "\n",
    "disability_df = pd.DataFrame(disability_df)\n",
    "\n",
    "#Adding an index column for future use\n",
    "disability_df['IDX'] = disability_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a250a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Count of 1s  Percentage of 1s\n",
      "IDX                               29890      12200.000000\n",
      "no_disability                       207         84.489796\n",
      "yes_disability                       28         11.428571\n",
      "dis_prefer_no_answer                  7          2.857143\n",
      "disability_unknown                    3          1.224490\n",
      "other_invisible_disability            1          0.408163\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(disability_df):\n",
    "    count_of_ones = disability_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / disability_df.count()) * 100  # Percentage of 1s\n",
    "    dis_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return dis_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "dis_desc_df = binary_count_and_percentage(disability_df)\n",
    "\n",
    "# Display the result\n",
    "print(dis_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecda7947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 245\n"
     ]
    }
   ],
   "source": [
    "#Creating a dataframe for binary values for minority for each demographic category\n",
    "#Specifying the new columns\n",
    "female=[]\n",
    "race_minority=[]\n",
    "age_minority=[]\n",
    "other_gender_minority=[]\n",
    "sex_minority=[]\n",
    "religion_minority=[]\n",
    "vet_minority=[]\n",
    "dis_minority=[]\n",
    "\n",
    "#Specifying what to include and exclude for the binary values for each new column\n",
    "for i in trimmed_data[\"Gender\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"cisgender female\" in i) or (\"female\" in i) and not (\"transgender female\" in i):\n",
    "        female.append(1)\n",
    "    else:\n",
    "        female.append(0)\n",
    "for i in trimmed_data[\"Race\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"indian\" in i) or (\"native american\" in i) or (\"black\" in i) or (\"middle eastern\" in i) \\\n",
    "    or (\"arab\" in i) or (\"asian\" in i) or (\"hispanic\" in i) or (\"islander\" in i) or (\"mixed\" in i):\n",
    "        race_minority.append(1)\n",
    "    else:\n",
    "        race_minority.append(0)\n",
    "for i in trimmed_data[\"Gender\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"nonbinary\" in i) or (\"transgender female\" in i):\n",
    "        other_gender_minority.append(1)\n",
    "    else:\n",
    "        other_gender_minority.append(0)\n",
    "for i in trimmed_data[\"Sexual_Orientation\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"asexual\" in i) or (\"bisexual\" in i) or (\"homosexual\" in i) or (\"pansexual\" in i) or (\"queer\" in i):\n",
    "        sex_minority.append(1)\n",
    "    else:\n",
    "        sex_minority.append(0)\n",
    "for i in trimmed_data[\"Religious_Affiliation\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"agnost\" in i) or (\"atheis\" in i) or (\"buddhis\" in i) or (\"hinduism\" in i) or (\"indigenous\" in i) \\\n",
    "    or (\"islam\" in i) or (\"judaism\" in i) or (\"spiritual\" in i) or (\"non\" in i): \n",
    "        religion_minority.append(1)\n",
    "    else:\n",
    "        religion_minority.append(0)\n",
    "for i in trimmed_data[\"Veteran_Status\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"i am a disabled veteran\" in i) or (\"i am a recently separated veteran\" in i) or \\\n",
    "    (\"i am an active duty wartime\" in i) or (\"i am an armed forces service medal veteran\" in i) \\\n",
    "    or (\"i am a va army national guard veteran\" in i) or (\"honorable discharge\" in i) \\\n",
    "    or (\"military spouse\" in i) or (\"veteran spouse\" in i) or (\"spouse of veteran\" in i) \\\n",
    "    or (\"wife of a veteran\" in i) or (\"dependent of active duty\" in i): \n",
    "        vet_minority.append(1)\n",
    "    else:\n",
    "        vet_minority.append(0)\n",
    "for i in trimmed_data[\"Disability_Status\"]:\n",
    "    i = str (i).lower()\n",
    "    if (\"i have a disability\" in i) or (\"invisible disability\" in i):\n",
    "        dis_minority.append(1)\n",
    "    else:\n",
    "        dis_minority.append(0)\n",
    "\n",
    "#Creating a new dataframe with the binary values\n",
    "diversity_df = {\n",
    "    'female' : female,\n",
    "    'race_minority' : race_minority,\n",
    "    'other_gender_minority' : other_gender_minority,\n",
    "    'sex_minority' : sex_minority,\n",
    "    'religion_minority' : religion_minority,\n",
    "    'vet_minority' : vet_minority,\n",
    "    'dis_minority' : dis_minority,\n",
    "\n",
    "}\n",
    "\n",
    "diversity_df = pd.DataFrame(diversity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b0d52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding an age minority column with binary values for the ordinal values protected by age discrimination laws.\n",
    "diversity_df['age_minority'] = age_ordinal_df['age_ordinal'].apply(lambda x: 1 if x in [4, 5, 6] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a558821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Count of 1s  Percentage of 1s\n",
      "female                         208         84.897959\n",
      "religion_minority              135         55.102041\n",
      "age_minority                    77         31.428571\n",
      "race_minority                   52         21.224490\n",
      "sex_minority                    42         17.142857\n",
      "dis_minority                    28         11.428571\n",
      "vet_minority                    12          4.897959\n",
      "other_gender_minority            2          0.816327\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "def binary_count_and_percentage(diviersity_df):\n",
    "    count_of_ones = diversity_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / diversity_df.count()) * 100  # Percentage of 1s\n",
    "    div_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return div_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "div_desc_df = binary_count_and_percentage(disability_df)\n",
    "\n",
    "# Display the result\n",
    "print(div_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15276cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a column for binary values if the row/respondent indicated at least one minority feature.\n",
    "#Specifying which columns to include in the binary count -- female not included.\n",
    "cols_to_include = ['race_minority', 'other_gender_minority', 'sex_minority', 'religion_minority', \\\n",
    "                  'vet_minority', 'dis_minority', 'age_minority']\n",
    "\n",
    "#Adding the binary values in the new column\n",
    "diversity_df['binary_diversity'] = diversity_df[cols_to_include].apply(lambda row: 1 if row.any() == 1 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff1d0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a column for binary values if the row/respondent indicated at least one minority feature.\n",
    "#Specifying which columns to include in the binary count -- female is included.\n",
    "cols_to_include = ['female', 'race_minority', 'other_gender_minority', 'sex_minority', 'religion_minority', \\\n",
    "                  'vet_minority', 'dis_minority', 'age_minority']\n",
    "\n",
    "#Adding the binary values to the new column\n",
    "diversity_df['binary_diversity_with_female'] = diversity_df[cols_to_include].apply \\\n",
    "(lambda row: 1 if row.any() == 1 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30c81f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a column for the sum of minority features reported\n",
    "#Specifying which columns to include in the binary count -- female not included.\n",
    "cols_to_include = ['race_minority', 'other_gender_minority', 'sex_minority', 'religion_minority', \\\n",
    "                  'vet_minority', 'dis_minority', 'age_minority']\n",
    "\n",
    "#Adding the ordinal values to the new column\n",
    "diversity_df['sum_diversity'] = diversity_df[cols_to_include].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8237b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a column for the sum of minority features reported.\n",
    "#Specifying which columns to include in the binary count -- female not included.\n",
    "cols_to_include = ['female', 'race_minority', 'other_gender_minority', 'sex_minority', 'religion_minority', \\\n",
    "                  'vet_minority', 'dis_minority', 'age_minority']\n",
    "\n",
    "#Adding the ordinal values to the new column\n",
    "diversity_df['sum_diversity_with_female'] = diversity_df[cols_to_include].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "189909f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a new column with ordinal values of 0 = no minority features reported, 1 = one reported, and 2 = 2+ reported\n",
    "diversity_df['three_diversity'] = diversity_df['sum_diversity'].apply(lambda x: 1 if x in [1] else 2 if x in \\\n",
    "                                                                      [2, 3, 4, 5] else 0)\n",
    "\n",
    "#Adding an index column for future use\n",
    "diversity_df['IDX'] = diversity_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ceee191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding another column with ordinal values for 0, 1, or 2+ minority features reported -- including female\n",
    "diversity_df['three_diversity_with_female'] = diversity_df['sum_diversity_with_female'].apply \\\n",
    "(lambda x: 1 if x in [1] else 2 if x in [2, 3, 4, 5] else 0)\n",
    "\n",
    "diversity_df['IDX'] = diversity_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "37c147da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Count of 1s  Percentage of 1s\n",
      "IDX                                 29890      12200.000000\n",
      "sum_diversity_with_female             556        226.938776\n",
      "three_diversity_with_female           434        177.142857\n",
      "sum_diversity                         348        142.040816\n",
      "three_diversity                       306        124.897959\n",
      "binary_diversity_with_female          240         97.959184\n",
      "female                                208         84.897959\n",
      "binary_diversity                      207         84.489796\n",
      "religion_minority                     135         55.102041\n",
      "age_minority                           77         31.428571\n",
      "race_minority                          52         21.224490\n",
      "sex_minority                           42         17.142857\n",
      "dis_minority                           28         11.428571\n",
      "vet_minority                           12          4.897959\n",
      "other_gender_minority                   2          0.816327\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate count and percentage of 1s for each column\n",
    "#NOTE: can disregard the sum_diversity and three_diversity columns here (and see next cells below).\n",
    "def binary_count_and_percentage(diviersity_df):\n",
    "    count_of_ones = diversity_df.sum()  # Count of 1s\n",
    "    percentage_of_ones = (count_of_ones / diversity_df.count()) * 100  # Percentage of 1s\n",
    "    div_desc_df = pd.DataFrame({\n",
    "        'Count of 1s': count_of_ones,\n",
    "        'Percentage of 1s': percentage_of_ones\n",
    "    })\n",
    "    return div_desc_df.sort_values(by='Count of 1s', ascending=False)  # Sort by count in descending order\n",
    "\n",
    "# Call the function\n",
    "div_desc_df = binary_count_and_percentage(diversity_df)\n",
    "\n",
    "# Display the result\n",
    "print(div_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "db41e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "1    108   44.081633\n",
      "2     62   25.306122\n",
      "0     38   15.510204\n",
      "3     33   13.469388\n",
      "4      3    1.224490\n",
      "5      1    0.408163\n"
     ]
    }
   ],
   "source": [
    "#Code to return the count and percentage of the sum_diversity column\n",
    "def get_count_percentage(diversity_df, sum_diversity):\n",
    "    \n",
    "# Get counts of each unique value\n",
    "    counts = diversity_df[sum_diversity].value_counts()\n",
    "    \n",
    "# Calculate the percentage\n",
    "    percentages = diversity_df[sum_diversity].value_counts(normalize=True) * 100\n",
    "    \n",
    "# Combine counts and percentages into a DataFrame\n",
    "    sum_div_desc_df = pd.DataFrame({\n",
    "        'Count': counts,\n",
    "        'Percentage': percentages\n",
    "    })\n",
    "    \n",
    "    return sum_div_desc_df\n",
    "\n",
    "sum_div_desc_df = get_count_percentage(diversity_df, 'sum_diversity')\n",
    "\n",
    "print(sum_div_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "93c699b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "2    104   42.448980\n",
      "3     62   25.306122\n",
      "1     46   18.775510\n",
      "4     24    9.795918\n",
      "0      5    2.040816\n",
      "5      4    1.632653\n"
     ]
    }
   ],
   "source": [
    "#Code to return the count and percentage of the sum_diversity_with_female column\n",
    "def get_count_percentage(diversity_df, sum_diversity_with_female):\n",
    "    \n",
    "# Get counts of each unique value\n",
    "    counts = diversity_df[sum_diversity_with_female].value_counts()\n",
    "    \n",
    "# Calculate the percentage\n",
    "    percentages = diversity_df[sum_diversity_with_female].value_counts(normalize=True) * 100\n",
    "    \n",
    "# Combine counts and percentages into a DataFrame\n",
    "    sum_div_female_desc_df = pd.DataFrame({\n",
    "        'Count': counts,\n",
    "        'Percentage': percentages\n",
    "    })\n",
    "    \n",
    "    return sum_div_female_desc_df\n",
    "\n",
    "sum_div_female_desc_df = get_count_percentage(diversity_df, 'sum_diversity_with_female')\n",
    "\n",
    "print(sum_div_female_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d87924c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "1    108   44.081633\n",
      "2     99   40.408163\n",
      "0     38   15.510204\n"
     ]
    }
   ],
   "source": [
    "#Code to return the count and percentage of the three_diversity column\n",
    "def get_count_percentage(diversity_df, three_diversity):\n",
    "    \n",
    "# Get counts of each unique value\n",
    "    counts = diversity_df[three_diversity].value_counts()\n",
    "    \n",
    "# Calculate the percentage\n",
    "    percentages = diversity_df[three_diversity].value_counts(normalize=True) * 100\n",
    "    \n",
    "# Combine counts and percentages into a DataFrame\n",
    "    three_div_desc_df = pd.DataFrame({\n",
    "        'Count': counts,\n",
    "        'Percentage': percentages\n",
    "    })\n",
    "    \n",
    "    return three_div_desc_df\n",
    "\n",
    "three_div_desc_df = get_count_percentage(diversity_df, 'three_diversity')\n",
    "\n",
    "print(three_div_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "91a6266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count  Percentage\n",
      "2    194   79.183673\n",
      "1     46   18.775510\n",
      "0      5    2.040816\n"
     ]
    }
   ],
   "source": [
    "#Code to return the count and percentage of the three_diversity_with_female column\n",
    "def get_count_percentage(diversity_df, three_diversity_with_female):\n",
    "    \n",
    "# Get counts of each unique value\n",
    "    counts = diversity_df[three_diversity_with_female].value_counts()\n",
    "    \n",
    "# Calculate the percentage\n",
    "    percentages = diversity_df[three_diversity_with_female].value_counts(normalize=True) * 100\n",
    "    \n",
    "# Combine counts and percentages into a DataFrame\n",
    "    three_div_female_desc_df = pd.DataFrame({\n",
    "        'Count': counts,\n",
    "        'Percentage': percentages\n",
    "    })\n",
    "    \n",
    "    return three_div_female_desc_df\n",
    "\n",
    "three_div_female_desc_df = get_count_percentage(diversity_df, 'three_diversity_with_female')\n",
    "\n",
    "print(three_div_female_desc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "147e2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging multiple dataframes to have all possible IVs and the diversity dataframe in one dataframe\n",
    "#Specifying the dataframes to include\n",
    "dfs = [ceo_df, org_size_df, service_settings_df, geo_df, position_category_df, position_duration_df, \\\n",
    "       degree_ordinal_df, training_df, certifications_df, diversity_df]\n",
    "\n",
    "#HOW DO I DESCRIBE THIS ONE? \n",
    "merged_div_df= dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_div_df=pd.merge(merged_div_df, df, on='IDX', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f37f77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing correlations for all features in the merged dataframe\n",
    "corr_merged_div_df=merged_div_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37c77bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:9: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  significant_corr = significant_corr.where(~pd.np.eye(significant_corr.shape[0], dtype=bool))\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n",
      "C:\\Users\\krglodowski\\AppData\\Local\\Temp\\ipykernel_23844\\1440623344.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_df['count'] = temp_df.sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column 1</th>\n",
       "      <th>Column 2</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ed_domain_unknown</td>\n",
       "      <td>ed_finance</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ed_finance</td>\n",
       "      <td>ed_domain_unknown</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setting_unknown</td>\n",
       "      <td>geo_unknown</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geo_unknown</td>\n",
       "      <td>setting_unknown</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.933948</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.933948</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ceo_unknown</td>\n",
       "      <td>ceo_finance</td>\n",
       "      <td>0.926928</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ceo_finance</td>\n",
       "      <td>ceo_unknown</td>\n",
       "      <td>0.926928</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.912834</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.912834</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.864913</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.864913</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>0.830281</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>0.830281</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.758926</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>0.758926</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>0.714304</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.714304</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.707356</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>0.707356</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.625809</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>0.625809</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>0.603687</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.603687</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>0.599356</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.599356</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sex_minority</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.560644</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>sex_minority</td>\n",
       "      <td>0.560644</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>binary_diversity_with_female</td>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>0.548694</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>binary_diversity_with_female</td>\n",
       "      <td>0.548694</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>direct_service</td>\n",
       "      <td>other_rbt_bcaba</td>\n",
       "      <td>0.545661</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>other_rbt_bcaba</td>\n",
       "      <td>direct_service</td>\n",
       "      <td>0.545661</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>sex_minority</td>\n",
       "      <td>0.530282</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sex_minority</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.530282</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ed_finance</td>\n",
       "      <td>ed_business</td>\n",
       "      <td>0.510284</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ed_business</td>\n",
       "      <td>ed_finance</td>\n",
       "      <td>0.510284</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ed_domain_unknown</td>\n",
       "      <td>ed_business</td>\n",
       "      <td>0.510284</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ed_business</td>\n",
       "      <td>ed_domain_unknown</td>\n",
       "      <td>0.510284</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>religion_minority</td>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>0.508180</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>religion_minority</td>\n",
       "      <td>0.508180</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>religion_minority</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>religion_minority</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.493304</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>religion_minority</td>\n",
       "      <td>0.475645</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>religion_minority</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.475645</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>religion_minority</td>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>0.474654</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>religion_minority</td>\n",
       "      <td>0.474654</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ed_hr</td>\n",
       "      <td>ed_business</td>\n",
       "      <td>0.467166</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ed_business</td>\n",
       "      <td>ed_hr</td>\n",
       "      <td>0.467166</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>religion_minority</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.457739</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>religion_minority</td>\n",
       "      <td>0.457739</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>sex_minority</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.453751</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>sex_minority</td>\n",
       "      <td>0.453751</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>ed_finance</td>\n",
       "      <td>ed_hr</td>\n",
       "      <td>0.448354</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ed_hr</td>\n",
       "      <td>ed_domain_unknown</td>\n",
       "      <td>0.448354</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ed_hr</td>\n",
       "      <td>ed_finance</td>\n",
       "      <td>0.448354</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ed_domain_unknown</td>\n",
       "      <td>ed_hr</td>\n",
       "      <td>0.448354</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ed_behavior_analysis</td>\n",
       "      <td>bcba</td>\n",
       "      <td>0.447819</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>bcba</td>\n",
       "      <td>ed_behavior_analysis</td>\n",
       "      <td>0.447819</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ed_social_work</td>\n",
       "      <td>licensed_sw</td>\n",
       "      <td>0.439790</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>licensed_sw</td>\n",
       "      <td>ed_social_work</td>\n",
       "      <td>0.439790</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>dis_minority</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.438354</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>dis_minority</td>\n",
       "      <td>0.438354</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>race_minority</td>\n",
       "      <td>0.412008</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>race_minority</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.412008</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>community</td>\n",
       "      <td>home</td>\n",
       "      <td>0.411533</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>home</td>\n",
       "      <td>community</td>\n",
       "      <td>0.411533</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>dis_minority</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.392969</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>dis_minority</td>\n",
       "      <td>0.392969</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>race_minority</td>\n",
       "      <td>0.381406</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>race_minority</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.381406</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>race_minority</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.368821</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>race_minority</td>\n",
       "      <td>0.368821</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>board_member</td>\n",
       "      <td>ed_hr</td>\n",
       "      <td>0.364557</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>ed_hr</td>\n",
       "      <td>board_member</td>\n",
       "      <td>0.364557</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>age_minority</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.359468</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>age_minority</td>\n",
       "      <td>0.359468</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>female</td>\n",
       "      <td>0.355750</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>female</td>\n",
       "      <td>three_diversity_with_female</td>\n",
       "      <td>0.355750</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>age_minority</td>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>0.349245</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>sum_diversity</td>\n",
       "      <td>age_minority</td>\n",
       "      <td>0.349245</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>degree_ordinal</td>\n",
       "      <td>bcba</td>\n",
       "      <td>0.346606</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>bcba</td>\n",
       "      <td>degree_ordinal</td>\n",
       "      <td>0.346606</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>other_cert_teaching</td>\n",
       "      <td>ed_education</td>\n",
       "      <td>0.344881</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ed_education</td>\n",
       "      <td>other_cert_teaching</td>\n",
       "      <td>0.344881</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ed_domain_other</td>\n",
       "      <td>slp</td>\n",
       "      <td>0.343218</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>slp</td>\n",
       "      <td>ed_domain_other</td>\n",
       "      <td>0.343218</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>binary_diversity_with_female</td>\n",
       "      <td>female</td>\n",
       "      <td>0.342224</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>female</td>\n",
       "      <td>binary_diversity_with_female</td>\n",
       "      <td>0.342224</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>community</td>\n",
       "      <td>school</td>\n",
       "      <td>0.337775</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>school</td>\n",
       "      <td>community</td>\n",
       "      <td>0.337775</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>binary_diversity_with_female</td>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>0.336878</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>binary_diversity</td>\n",
       "      <td>binary_diversity_with_female</td>\n",
       "      <td>0.336878</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>binary_diversity_with_female</td>\n",
       "      <td>0.329432</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>binary_diversity_with_female</td>\n",
       "      <td>sum_diversity_with_female</td>\n",
       "      <td>0.329432</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>bcba</td>\n",
       "      <td>lba</td>\n",
       "      <td>0.311738</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>lba</td>\n",
       "      <td>bcba</td>\n",
       "      <td>0.311738</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>vocational_program</td>\n",
       "      <td>residential_group_home</td>\n",
       "      <td>0.311505</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>residential_group_home</td>\n",
       "      <td>vocational_program</td>\n",
       "      <td>0.311505</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>dis_minority</td>\n",
       "      <td>three_diversity</td>\n",
       "      <td>0.309818</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>three_diversity</td>\n",
       "      <td>dis_minority</td>\n",
       "      <td>0.309818</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>position_duration</td>\n",
       "      <td>age_minority</td>\n",
       "      <td>0.304022</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>age_minority</td>\n",
       "      <td>position_duration</td>\n",
       "      <td>0.304022</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>org_size</td>\n",
       "      <td>northeast_us</td>\n",
       "      <td>0.302970</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>northeast_us</td>\n",
       "      <td>org_size</td>\n",
       "      <td>0.302970</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ceo_behavior_analysis</td>\n",
       "      <td>org_size</td>\n",
       "      <td>-0.306734</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>org_size</td>\n",
       "      <td>ceo_behavior_analysis</td>\n",
       "      <td>-0.306734</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>executive_member</td>\n",
       "      <td>director</td>\n",
       "      <td>-0.314146</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>director</td>\n",
       "      <td>executive_member</td>\n",
       "      <td>-0.314146</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>degree_ordinal</td>\n",
       "      <td>other_rbt_bcaba</td>\n",
       "      <td>-0.337614</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>other_rbt_bcaba</td>\n",
       "      <td>degree_ordinal</td>\n",
       "      <td>-0.337614</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>director</td>\n",
       "      <td>case_manager</td>\n",
       "      <td>-0.375915</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>case_manager</td>\n",
       "      <td>director</td>\n",
       "      <td>-0.375915</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>home</td>\n",
       "      <td>residential_group_home</td>\n",
       "      <td>-0.386755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>residential_group_home</td>\n",
       "      <td>home</td>\n",
       "      <td>-0.386755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>bcba</td>\n",
       "      <td>direct_service</td>\n",
       "      <td>-0.424614</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>direct_service</td>\n",
       "      <td>bcba</td>\n",
       "      <td>-0.424614</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>ed_behavior_analysis</td>\n",
       "      <td>cert_unknown</td>\n",
       "      <td>-0.452470</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>cert_unknown</td>\n",
       "      <td>ed_behavior_analysis</td>\n",
       "      <td>-0.452470</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>cert_unknown</td>\n",
       "      <td>bcba</td>\n",
       "      <td>-0.482722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>bcba</td>\n",
       "      <td>cert_unknown</td>\n",
       "      <td>-0.482722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>ceo_business</td>\n",
       "      <td>ceo_behavior_analysis</td>\n",
       "      <td>-0.503818</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>ceo_behavior_analysis</td>\n",
       "      <td>ceo_business</td>\n",
       "      <td>-0.503818</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>bcba</td>\n",
       "      <td>other_rbt_bcaba</td>\n",
       "      <td>-0.670977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>other_rbt_bcaba</td>\n",
       "      <td>bcba</td>\n",
       "      <td>-0.670977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Column 1                      Column 2  Correlation  \\\n",
       "0               ed_domain_unknown                    ed_finance     1.000000   \n",
       "1                      ed_finance             ed_domain_unknown     1.000000   \n",
       "2                 setting_unknown                   geo_unknown     1.000000   \n",
       "3                     geo_unknown               setting_unknown     1.000000   \n",
       "4                   sum_diversity     sum_diversity_with_female     0.933948   \n",
       "5       sum_diversity_with_female                 sum_diversity     0.933948   \n",
       "6                     ceo_unknown                   ceo_finance     0.926928   \n",
       "7                     ceo_finance                   ceo_unknown     0.926928   \n",
       "8                   sum_diversity               three_diversity     0.912834   \n",
       "9                 three_diversity                 sum_diversity     0.912834   \n",
       "10      sum_diversity_with_female               three_diversity     0.864913   \n",
       "11                three_diversity     sum_diversity_with_female     0.864913   \n",
       "12    three_diversity_with_female              binary_diversity     0.830281   \n",
       "13               binary_diversity   three_diversity_with_female     0.830281   \n",
       "14               binary_diversity               three_diversity     0.758926   \n",
       "15                three_diversity              binary_diversity     0.758926   \n",
       "16      sum_diversity_with_female   three_diversity_with_female     0.714304   \n",
       "17    three_diversity_with_female     sum_diversity_with_female     0.714304   \n",
       "18    three_diversity_with_female               three_diversity     0.707356   \n",
       "19                three_diversity   three_diversity_with_female     0.707356   \n",
       "20               binary_diversity                 sum_diversity     0.625809   \n",
       "21                  sum_diversity              binary_diversity     0.625809   \n",
       "22      sum_diversity_with_female              binary_diversity     0.603687   \n",
       "23               binary_diversity     sum_diversity_with_female     0.603687   \n",
       "24                  sum_diversity   three_diversity_with_female     0.599356   \n",
       "25    three_diversity_with_female                 sum_diversity     0.599356   \n",
       "26                   sex_minority                 sum_diversity     0.560644   \n",
       "27                  sum_diversity                  sex_minority     0.560644   \n",
       "28   binary_diversity_with_female   three_diversity_with_female     0.548694   \n",
       "29    three_diversity_with_female  binary_diversity_with_female     0.548694   \n",
       "30                 direct_service               other_rbt_bcaba     0.545661   \n",
       "31                other_rbt_bcaba                direct_service     0.545661   \n",
       "32      sum_diversity_with_female                  sex_minority     0.530282   \n",
       "33                   sex_minority     sum_diversity_with_female     0.530282   \n",
       "34                     ed_finance                   ed_business     0.510284   \n",
       "35                    ed_business                    ed_finance     0.510284   \n",
       "36              ed_domain_unknown                   ed_business     0.510284   \n",
       "37                    ed_business             ed_domain_unknown     0.510284   \n",
       "38              religion_minority   three_diversity_with_female     0.508180   \n",
       "39    three_diversity_with_female             religion_minority     0.508180   \n",
       "40                three_diversity             religion_minority     0.493304   \n",
       "41              religion_minority               three_diversity     0.493304   \n",
       "42      sum_diversity_with_female             religion_minority     0.475645   \n",
       "43              religion_minority     sum_diversity_with_female     0.475645   \n",
       "44              religion_minority              binary_diversity     0.474654   \n",
       "45               binary_diversity             religion_minority     0.474654   \n",
       "46                          ed_hr                   ed_business     0.467166   \n",
       "47                    ed_business                         ed_hr     0.467166   \n",
       "48              religion_minority                 sum_diversity     0.457739   \n",
       "49                  sum_diversity             religion_minority     0.457739   \n",
       "50                   sex_minority               three_diversity     0.453751   \n",
       "51                three_diversity                  sex_minority     0.453751   \n",
       "52                     ed_finance                         ed_hr     0.448354   \n",
       "53                          ed_hr             ed_domain_unknown     0.448354   \n",
       "54                          ed_hr                    ed_finance     0.448354   \n",
       "55              ed_domain_unknown                         ed_hr     0.448354   \n",
       "56           ed_behavior_analysis                          bcba     0.447819   \n",
       "57                           bcba          ed_behavior_analysis     0.447819   \n",
       "58                 ed_social_work                   licensed_sw     0.439790   \n",
       "59                    licensed_sw                ed_social_work     0.439790   \n",
       "60                   dis_minority                 sum_diversity     0.438354   \n",
       "61                  sum_diversity                  dis_minority     0.438354   \n",
       "62                  sum_diversity                 race_minority     0.412008   \n",
       "63                  race_minority                 sum_diversity     0.412008   \n",
       "64                      community                          home     0.411533   \n",
       "65                           home                     community     0.411533   \n",
       "66                   dis_minority     sum_diversity_with_female     0.392969   \n",
       "67      sum_diversity_with_female                  dis_minority     0.392969   \n",
       "68      sum_diversity_with_female                 race_minority     0.381406   \n",
       "69                  race_minority     sum_diversity_with_female     0.381406   \n",
       "70                  race_minority               three_diversity     0.368821   \n",
       "71                three_diversity                 race_minority     0.368821   \n",
       "72                   board_member                         ed_hr     0.364557   \n",
       "73                          ed_hr                  board_member     0.364557   \n",
       "74                   age_minority               three_diversity     0.359468   \n",
       "75                three_diversity                  age_minority     0.359468   \n",
       "76    three_diversity_with_female                        female     0.355750   \n",
       "77                         female   three_diversity_with_female     0.355750   \n",
       "78                   age_minority                 sum_diversity     0.349245   \n",
       "79                  sum_diversity                  age_minority     0.349245   \n",
       "80                 degree_ordinal                          bcba     0.346606   \n",
       "81                           bcba                degree_ordinal     0.346606   \n",
       "82            other_cert_teaching                  ed_education     0.344881   \n",
       "83                   ed_education           other_cert_teaching     0.344881   \n",
       "84                ed_domain_other                           slp     0.343218   \n",
       "85                            slp               ed_domain_other     0.343218   \n",
       "86   binary_diversity_with_female                        female     0.342224   \n",
       "87                         female  binary_diversity_with_female     0.342224   \n",
       "88                      community                        school     0.337775   \n",
       "89                         school                     community     0.337775   \n",
       "90   binary_diversity_with_female              binary_diversity     0.336878   \n",
       "91               binary_diversity  binary_diversity_with_female     0.336878   \n",
       "92      sum_diversity_with_female  binary_diversity_with_female     0.329432   \n",
       "93   binary_diversity_with_female     sum_diversity_with_female     0.329432   \n",
       "94                           bcba                           lba     0.311738   \n",
       "95                            lba                          bcba     0.311738   \n",
       "96             vocational_program        residential_group_home     0.311505   \n",
       "97         residential_group_home            vocational_program     0.311505   \n",
       "98                   dis_minority               three_diversity     0.309818   \n",
       "99                three_diversity                  dis_minority     0.309818   \n",
       "100             position_duration                  age_minority     0.304022   \n",
       "101                  age_minority             position_duration     0.304022   \n",
       "102                      org_size                  northeast_us     0.302970   \n",
       "103                  northeast_us                      org_size     0.302970   \n",
       "104         ceo_behavior_analysis                      org_size    -0.306734   \n",
       "105                      org_size         ceo_behavior_analysis    -0.306734   \n",
       "106              executive_member                      director    -0.314146   \n",
       "107                      director              executive_member    -0.314146   \n",
       "108                degree_ordinal               other_rbt_bcaba    -0.337614   \n",
       "109               other_rbt_bcaba                degree_ordinal    -0.337614   \n",
       "110                      director                  case_manager    -0.375915   \n",
       "111                  case_manager                      director    -0.375915   \n",
       "112                          home        residential_group_home    -0.386755   \n",
       "113        residential_group_home                          home    -0.386755   \n",
       "114                          bcba                direct_service    -0.424614   \n",
       "115                direct_service                          bcba    -0.424614   \n",
       "116          ed_behavior_analysis                  cert_unknown    -0.452470   \n",
       "117                  cert_unknown          ed_behavior_analysis    -0.452470   \n",
       "118                  cert_unknown                          bcba    -0.482722   \n",
       "119                          bcba                  cert_unknown    -0.482722   \n",
       "120                  ceo_business         ceo_behavior_analysis    -0.503818   \n",
       "121         ceo_behavior_analysis                  ceo_business    -0.503818   \n",
       "122                          bcba               other_rbt_bcaba    -0.670977   \n",
       "123               other_rbt_bcaba                          bcba    -0.670977   \n",
       "\n",
       "     count  \n",
       "0        6  \n",
       "1        6  \n",
       "2        1  \n",
       "3        1  \n",
       "4       13  \n",
       "5       13  \n",
       "6       13  \n",
       "7       13  \n",
       "8      108  \n",
       "9      108  \n",
       "10      13  \n",
       "11      13  \n",
       "12      13  \n",
       "13      13  \n",
       "14     108  \n",
       "15     108  \n",
       "16      46  \n",
       "17      46  \n",
       "18      13  \n",
       "19      13  \n",
       "20     108  \n",
       "21     108  \n",
       "22      13  \n",
       "23      13  \n",
       "24      13  \n",
       "25      13  \n",
       "26      47  \n",
       "27      47  \n",
       "28      46  \n",
       "29      46  \n",
       "30      10  \n",
       "31      10  \n",
       "32     103  \n",
       "33     103  \n",
       "34       5  \n",
       "35       5  \n",
       "36       5  \n",
       "37       5  \n",
       "38      63  \n",
       "39      63  \n",
       "40      82  \n",
       "41      82  \n",
       "42      43  \n",
       "43      43  \n",
       "44     135  \n",
       "45     135  \n",
       "46       5  \n",
       "47       5  \n",
       "48      76  \n",
       "49      76  \n",
       "50      61  \n",
       "51      61  \n",
       "52       3  \n",
       "53       3  \n",
       "54       3  \n",
       "55       3  \n",
       "56     213  \n",
       "57     213  \n",
       "58       2  \n",
       "59       2  \n",
       "60      59  \n",
       "61      59  \n",
       "62      55  \n",
       "63      55  \n",
       "64      71  \n",
       "65      71  \n",
       "66     101  \n",
       "67     101  \n",
       "68      95  \n",
       "69      95  \n",
       "70      73  \n",
       "71      73  \n",
       "72       2  \n",
       "73       2  \n",
       "74      80  \n",
       "75      80  \n",
       "76      52  \n",
       "77      52  \n",
       "78      63  \n",
       "79      63  \n",
       "80      12  \n",
       "81      12  \n",
       "82      18  \n",
       "83      18  \n",
       "84       2  \n",
       "85       2  \n",
       "86     208  \n",
       "87     208  \n",
       "88      67  \n",
       "89      67  \n",
       "90     207  \n",
       "91     207  \n",
       "92      46  \n",
       "93      46  \n",
       "94     114  \n",
       "95     114  \n",
       "96      10  \n",
       "97      10  \n",
       "98      79  \n",
       "99      79  \n",
       "100    105  \n",
       "101    105  \n",
       "102     71  \n",
       "103     71  \n",
       "104     44  \n",
       "105     44  \n",
       "106      9  \n",
       "107      9  \n",
       "108    174  \n",
       "109    174  \n",
       "110     14  \n",
       "111     14  \n",
       "112      0  \n",
       "113      0  \n",
       "114      8  \n",
       "115      8  \n",
       "116      4  \n",
       "117      4  \n",
       "118      0  \n",
       "119      0  \n",
       "120     14  \n",
       "121     14  \n",
       "122      0  \n",
       "123      0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I NEED HELP DESCRIBING THIS CELL. \n",
    "# Assuming df is your DataFrame -- ???????????? \n",
    "correlation_matrix = merged_div_df.corr()\n",
    "\n",
    "# Filter the correlation matrix to get values greater than 0.30 or less than -0.30 -- stat sig level/threshold\n",
    "significant_corr = correlation_matrix[(correlation_matrix > 0.30) | (correlation_matrix < -0.30)]\n",
    "\n",
    "# Remove self-correlations (correlation of a column with itself)\n",
    "significant_corr = significant_corr.where(~pd.np.eye(significant_corr.shape[0], dtype=bool))\n",
    "\n",
    "# Get the list of column pairs with significant correlations\n",
    "significant_pairs = significant_corr.stack().reset_index()\n",
    "significant_pairs.columns = ['Column 1', 'Column 2', 'Correlation']\n",
    "\n",
    "# Sort by absolute correlation value (optional)\n",
    "significant_pairs = significant_pairs.sort_values(by='Correlation', ascending=False).reset_index(drop=True)\n",
    "\n",
    "#??????????????????????????????\n",
    "count_list = []\n",
    "for row in range(len(significant_pairs)):\n",
    "    Col1 = significant_pairs['Column 1'][row]\n",
    "    Col2 = significant_pairs['Column 2'][row]\n",
    "    temp_df = merged_div_df[[Col1, Col2]]\n",
    "    temp_df['count'] = temp_df.sum(axis=1)\n",
    "    temp_df = temp_df[temp_df['count'] == 2]\n",
    "    count_list.append(len(temp_df))\n",
    "\n",
    "significant_pairs['count'] = count_list\n",
    "\n",
    "significant_pairs\n",
    "\n",
    "#?????????????????????????????????????????? Should I keep this information (here or somewhere else???)\n",
    "#10-20% would be 24 to 48+\n",
    "#QUESTION: What's the relevance of the 10-20% threshold again? \n",
    "#stat sig level/threshold\n",
    "\n",
    "#positive correlations:\n",
    "#sum_diversity and three_diversity -- see modeling plan below \n",
    "#binary_diversity and three_diversity -- see modeling plan below\n",
    "#sum_diversity_with_female and three_diversity_with_female -- see modeling plan below\n",
    "#binary_diversity and sum_diversity -- see modeling plan\n",
    "#sex_minority and sum_diversity -- see modeling plan\n",
    "#binary_diversity_with_female and three_diversity_with_female -- see modeling plan\n",
    "#sum_diversity_with_female and sex_minority -- see modeling plan\n",
    "#religion_minority and #sum_diversity_with_female -- see modeling plan\n",
    "#religion_minority and binary_diversity -- see modeling plan\n",
    "#religion_minority and sum_diversity -- see modeling plan\n",
    "#sex_minority and three_diversity -- see modeling plan\n",
    "#ed_behavior_analysis and bcba -- omit ed_behavior_analysis from all modeling df\n",
    "#dis_minority and sum_diversity -- see modeling plan\n",
    "#sum_diversity and race_minority -- see modeling plan\n",
    "#community and home -- omit community from all modeling df\n",
    "#dis_minority and sum_diversity_with_female -- see modeling plan\n",
    "#sum_diversity_with_female and race_minority -- see modeling plan\n",
    "#race_minority and three_diversity -- see modeling plan\n",
    "#age_minority and three_diversity -- see modeling plan\n",
    "#three_diversity_with_female and female -- see modeling plan\n",
    "#age_minority and sum_diversity -- see modeling plan\n",
    "#binary_diversity_with_female and female -- see modeling plan\n",
    "#community and school -- omit community from all modeling df \n",
    "#sum_diversity_with_female and binary_diversity_with_female -- see modeling plan\n",
    "#bcba and lba -- omit lba from all modeling df\n",
    "#dis_minority and three_diversity -- see modeling plan\n",
    "#position_duration and age_minority - omit position_duration from all modeling df\n",
    "#org_size and northeast_us -- omit org_size from all modeling df\n",
    "\n",
    "\n",
    "#negative correlation:\n",
    "#ceo_behavior_analys and org_size -- omit org_size for all modeling df\n",
    "#degree_ordinal and other_rbt_bcaba -- omit other_rbt_bcaba for all modeling df \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MY PLANS FOR MODELING FOR THE MULTICOLLINIARITY ABOVE: \n",
    "#One df with all minority columns (and none of the diversity aggregate columns)\n",
    "#One df with no minority columns (except the female column) and binary_diversity column\n",
    "#One df with no minority columns and binary_diversity_with_female column (and no female column)\n",
    "#One df with no minority columns (except the female column) and sum_diversity column\n",
    "#One df with no minority columns and sum_diversity_with_female column (and no female column)\n",
    "#One df with no minority columns (except the femal column) and three_diversity column\n",
    "#One df with no minority columns and three_diversity_with_female column (and no female column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8c174805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for org_size vs female:\n",
      "female     0   1\n",
      "org_size        \n",
      "1          5  22\n",
      "2         13  67\n",
      "3          6  47\n",
      "4          4  32\n",
      "5          1  18\n",
      "6          5  13\n",
      "7          3   4\n",
      "8          0   5\n",
      "\n",
      "Row-normalized proportions for org_size vs female:\n",
      "female           0         1\n",
      "org_size                    \n",
      "1         0.185185  0.814815\n",
      "2         0.162500  0.837500\n",
      "3         0.113208  0.886792\n",
      "4         0.111111  0.888889\n",
      "5         0.052632  0.947368\n",
      "6         0.277778  0.722222\n",
      "7         0.428571  0.571429\n",
      "8         0.000000  1.000000\n",
      "\n",
      "Chi-Square Statistic for org_size vs female: 9.324109382313356\n",
      "P-Value: 0.23021623854643836\n",
      "Degrees of Freedom: 7\n",
      "\n",
      "Contingency Table for degree_ordinal vs female:\n",
      "female           0    1\n",
      "degree_ordinal         \n",
      "-1               1    8\n",
      " 0               0    1\n",
      " 1               2   12\n",
      " 2              20  149\n",
      " 3              14   38\n",
      "\n",
      "Row-normalized proportions for degree_ordinal vs female:\n",
      "female                 0         1\n",
      "degree_ordinal                    \n",
      "-1              0.111111  0.888889\n",
      " 0              0.000000  1.000000\n",
      " 1              0.142857  0.857143\n",
      " 2              0.118343  0.881657\n",
      " 3              0.269231  0.730769\n",
      "\n",
      "Chi-Square Statistic for degree_ordinal vs female: 7.506588794724884\n",
      "P-Value: 0.11141910552531153\n",
      "Degrees of Freedom: 4\n",
      "\n",
      "Contingency Table for case_manager vs female:\n",
      "female         0    1\n",
      "case_manager         \n",
      "0             27  135\n",
      "1             10   73\n",
      "\n",
      "Row-normalized proportions for case_manager vs female:\n",
      "female               0         1\n",
      "case_manager                    \n",
      "0             0.166667  0.833333\n",
      "1             0.120482  0.879518\n",
      "\n",
      "Chi-Square Statistic for case_manager vs female: 0.9130427635698711\n",
      "P-Value: 0.3393083709580196\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for director vs female:\n",
      "female     0    1\n",
      "director         \n",
      "0         23  117\n",
      "1         14   91\n",
      "\n",
      "Row-normalized proportions for director vs female:\n",
      "female           0         1\n",
      "director                    \n",
      "0         0.164286  0.835714\n",
      "1         0.133333  0.866667\n",
      "\n",
      "Chi-Square Statistic for director vs female: 0.44833896396396455\n",
      "P-Value: 0.5031248111209585\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for executive_member vs female:\n",
      "female             0    1\n",
      "executive_member         \n",
      "0                 22  164\n",
      "1                 15   44\n",
      "\n",
      "Row-normalized proportions for executive_member vs female:\n",
      "female                   0         1\n",
      "executive_member                    \n",
      "0                 0.118280  0.881720\n",
      "1                 0.254237  0.745763\n",
      "\n",
      "Chi-Square Statistic for executive_member vs female: 6.457638296074601\n",
      "P-Value: 0.011047638331163845\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for direct_service vs female:\n",
      "female           0    1\n",
      "direct_service         \n",
      "0               33  192\n",
      "1                4   16\n",
      "\n",
      "Row-normalized proportions for direct_service vs female:\n",
      "female                 0         1\n",
      "direct_service                    \n",
      "0               0.146667  0.853333\n",
      "1               0.200000  0.800000\n",
      "\n",
      "Chi-Square Statistic for direct_service vs female: 0.4074844074844077\n",
      "P-Value: 0.5232490901176128\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_behavior_analysis vs female:\n",
      "female                  0    1\n",
      "ceo_behavior_analysis         \n",
      "0                      18   97\n",
      "1                      19  111\n",
      "\n",
      "Row-normalized proportions for ceo_behavior_analysis vs female:\n",
      "female                        0         1\n",
      "ceo_behavior_analysis                    \n",
      "0                      0.156522  0.843478\n",
      "1                      0.146154  0.853846\n",
      "\n",
      "Chi-Square Statistic for ceo_behavior_analysis vs female: 0.0511591392653265\n",
      "P-Value: 0.8210584219380375\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_business vs female:\n",
      "female         0    1\n",
      "ceo_business         \n",
      "0             25  139\n",
      "1             12   69\n",
      "\n",
      "Row-normalized proportions for ceo_business vs female:\n",
      "female               0         1\n",
      "ceo_business                    \n",
      "0             0.152439  0.847561\n",
      "1             0.148148  0.851852\n",
      "\n",
      "Chi-Square Statistic for ceo_business vs female: 0.007786133929426614\n",
      "P-Value: 0.9296866629902104\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_psychology vs female:\n",
      "female           0    1\n",
      "ceo_psychology         \n",
      "0               31  168\n",
      "1                6   40\n",
      "\n",
      "Row-normalized proportions for ceo_psychology vs female:\n",
      "female                 0         1\n",
      "ceo_psychology                    \n",
      "0               0.155779  0.844221\n",
      "1               0.130435  0.869565\n",
      "\n",
      "Chi-Square Statistic for ceo_psychology vs female: 0.1871828636084696\n",
      "P-Value: 0.6652717117914178\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_education vs female:\n",
      "female          0    1\n",
      "ceo_education         \n",
      "0              32  171\n",
      "1               5   37\n",
      "\n",
      "Row-normalized proportions for ceo_education vs female:\n",
      "female                0         1\n",
      "ceo_education                    \n",
      "0              0.157635  0.842365\n",
      "1              0.119048  0.880952\n",
      "\n",
      "Chi-Square Statistic for ceo_education vs female: 0.4041545690252585\n",
      "P-Value: 0.5249514266149047\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_other_allied vs female:\n",
      "female             0    1\n",
      "ceo_other_allied         \n",
      "0                 29  193\n",
      "1                  8   15\n",
      "\n",
      "Row-normalized proportions for ceo_other_allied vs female:\n",
      "female                   0         1\n",
      "ceo_other_allied                    \n",
      "0                 0.130631  0.869369\n",
      "1                 0.347826  0.652174\n",
      "\n",
      "Chi-Square Statistic for ceo_other_allied vs female: 7.668021239738337\n",
      "P-Value: 0.005620806393712319\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for midwest_us vs female:\n",
      "female       0    1\n",
      "midwest_us         \n",
      "0           24  157\n",
      "1           13   51\n",
      "\n",
      "Row-normalized proportions for midwest_us vs female:\n",
      "female             0         1\n",
      "midwest_us                    \n",
      "0           0.132597  0.867403\n",
      "1           0.203125  0.796875\n",
      "\n",
      "Chi-Square Statistic for midwest_us vs female: 1.8343685233090883\n",
      "P-Value: 0.17561242757471174\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for south_us vs female:\n",
      "female     0    1\n",
      "south_us         \n",
      "0         23  126\n",
      "1         14   82\n",
      "\n",
      "Row-normalized proportions for south_us vs female:\n",
      "female           0         1\n",
      "south_us                    \n",
      "0         0.154362  0.845638\n",
      "1         0.145833  0.854167\n",
      "\n",
      "Chi-Square Statistic for south_us vs female: 0.03312555666792254\n",
      "P-Value: 0.855579369787767\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for west_us vs female:\n",
      "female    0    1\n",
      "west_us         \n",
      "0        27  155\n",
      "1        10   53\n",
      "\n",
      "Row-normalized proportions for west_us vs female:\n",
      "female          0         1\n",
      "west_us                    \n",
      "0        0.148352  0.851648\n",
      "1        0.158730  0.841270\n",
      "\n",
      "Chi-Square Statistic for west_us vs female: 0.03931723883646983\n",
      "P-Value: 0.8428214998334888\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for northeast_us vs female:\n",
      "female         0    1\n",
      "northeast_us         \n",
      "0             26  155\n",
      "1             11   53\n",
      "\n",
      "Row-normalized proportions for northeast_us vs female:\n",
      "female               0         1\n",
      "northeast_us                    \n",
      "0             0.143646  0.856354\n",
      "1             0.171875  0.828125\n",
      "\n",
      "Chi-Square Statistic for northeast_us vs female: 0.29385831351365727\n",
      "P-Value: 0.5877585751530893\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for other_geo vs female:\n",
      "female      0    1\n",
      "other_geo         \n",
      "0          36  197\n",
      "1           1   11\n",
      "\n",
      "Row-normalized proportions for other_geo vs female:\n",
      "female            0         1\n",
      "other_geo                    \n",
      "0          0.154506  0.845494\n",
      "1          0.083333  0.916667\n",
      "\n",
      "Chi-Square Statistic for other_geo vs female: 0.45088938496932074\n",
      "P-Value: 0.5019129019699038\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for org_size vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for degree_ordinal vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for case_manager vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for director vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for executive_member vs female: 0.17676221329862152\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for direct_service vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_behavior_analysis vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_business vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_psychology vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_education vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_other_allied vs female: 0.0899329022993971\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for midwest_us vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for south_us vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for west_us vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for northeast_us vs female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for other_geo vs female: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Is there a difference in female between groups for: geo_region? position? degree? ceo background? org size? \n",
    "# All the same as above but for the various aggregate diversity measures? \n",
    "\n",
    "# merged_div_df -- the df with all possible IVs, minority columns, and diversity aggregate columns\n",
    "\n",
    "#How do I describe this one?????????????????????? and is the above important to keep (here or somewhere else?)\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Target column to compare against other categorical variables\n",
    "target_col = 'female'\n",
    "\n",
    "# List of other columns to compare\n",
    "columns_to_compare = ['org_size','degree_ordinal', 'case_manager', 'director', 'executive_member', \\\n",
    "                     'direct_service', 'ceo_behavior_analysis', 'ceo_business', 'ceo_psychology', \\\n",
    "                     'ceo_education', 'ceo_other_allied', 'midwest_us', 'south_us', 'west_us', 'northeast_us', \\\n",
    "                     'other_geo']\n",
    "\n",
    "# Store p-values for Bonferroni correction later\n",
    "p_values = []\n",
    "\n",
    "# Loop through columns and perform chi-square tests\n",
    "for col in columns_to_compare:\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(merged_div_df[col], merged_div_df[target_col])\n",
    "    \n",
    "    # Normalize the proportions row-wise\n",
    "    row_normalized = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    \n",
    "    print(f\"\\nContingency Table for {col} vs {target_col}:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    print(f\"\\nRow-normalized proportions for {col} vs {target_col}:\")\n",
    "    print(row_normalized)\n",
    "\n",
    "    # Run the Chi-Square test using statsmodels\n",
    "    chi2_result = sm.stats.Table(contingency_table).test_nominal_association()\n",
    "\n",
    "    # Output the test results\n",
    "    print(f\"\\nChi-Square Statistic for {col} vs {target_col}: {chi2_result.statistic}\")\n",
    "    print(f\"P-Value: {chi2_result.pvalue}\")\n",
    "    print(f\"Degrees of Freedom: {chi2_result.df}\")\n",
    "\n",
    "    # Store p-values for Bonferroni correction\n",
    "    p_values.append(chi2_result.pvalue)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "adjusted_p_values = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Output the adjusted p-values\n",
    "for col, adj_p in zip(columns_to_compare, adjusted_p_values):\n",
    "    print(f\"\\nAdjusted P-Value (Bonferroni) for {col} vs {target_col}: {adj_p}\")\n",
    "\n",
    "##?????????????????????????????????????????????????? keep these (here or somewhere else??)  \n",
    "#executive_member vs female - but adjusted p-value is .18\n",
    "#ceo_other_allied vs female - but adjusted p-value is .09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dfde646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for org_size vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "org_size                   \n",
      "1                 4  10  13\n",
      "2                 6  37  37\n",
      "3                15  23  15\n",
      "4                 7  15  14\n",
      "5                 1  12   6\n",
      "6                 4   6   8\n",
      "7                 1   2   4\n",
      "8                 0   3   2\n",
      "\n",
      "Row-normalized proportions for org_size vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "org_size                                     \n",
      "1                0.148148  0.370370  0.481481\n",
      "2                0.075000  0.462500  0.462500\n",
      "3                0.283019  0.433962  0.283019\n",
      "4                0.194444  0.416667  0.388889\n",
      "5                0.052632  0.631579  0.315789\n",
      "6                0.222222  0.333333  0.444444\n",
      "7                0.142857  0.285714  0.571429\n",
      "8                0.000000  0.600000  0.400000\n",
      "\n",
      "Chi-Square Statistic for org_size vs three_diversity: 18.082717433932523\n",
      "P-Value: 0.20303938975991098\n",
      "Degrees of Freedom: 14\n",
      "\n",
      "Contingency Table for degree_ordinal vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "degree_ordinal             \n",
      "-1                1   1   7\n",
      " 0                0   1   0\n",
      " 1                4   6   4\n",
      " 2               27  75  67\n",
      " 3                6  25  21\n",
      "\n",
      "Row-normalized proportions for degree_ordinal vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "degree_ordinal                               \n",
      "-1               0.111111  0.111111  0.777778\n",
      " 0               0.000000  1.000000  0.000000\n",
      " 1               0.285714  0.428571  0.285714\n",
      " 2               0.159763  0.443787  0.396450\n",
      " 3               0.115385  0.480769  0.403846\n",
      "\n",
      "Chi-Square Statistic for degree_ordinal vs three_diversity: 8.470758831921332\n",
      "P-Value: 0.38888555522151813\n",
      "Degrees of Freedom: 8\n",
      "\n",
      "Contingency Table for case_manager vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "case_manager               \n",
      "0                28  70  64\n",
      "1                10  38  35\n",
      "\n",
      "Row-normalized proportions for case_manager vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "case_manager                                 \n",
      "0                0.172840  0.432099  0.395062\n",
      "1                0.120482  0.457831  0.421687\n",
      "\n",
      "Chi-Square Statistic for case_manager vs three_diversity: 1.1487129001827165\n",
      "P-Value: 0.5630671140228043\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for director vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "director                   \n",
      "0                19  56  65\n",
      "1                19  52  34\n",
      "\n",
      "Row-normalized proportions for director vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "director                                     \n",
      "0                0.135714  0.400000  0.464286\n",
      "1                0.180952  0.495238  0.323810\n",
      "\n",
      "Chi-Square Statistic for director vs three_diversity: 4.956369248035917\n",
      "P-Value: 0.08389538910780137\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for executive_member vs three_diversity:\n",
      "three_diversity    0   1   2\n",
      "executive_member            \n",
      "0                 29  89  68\n",
      "1                  9  19  31\n",
      "\n",
      "Row-normalized proportions for executive_member vs three_diversity:\n",
      "three_diversity          0         1         2\n",
      "executive_member                              \n",
      "0                 0.155914  0.478495  0.365591\n",
      "1                 0.152542  0.322034  0.525424\n",
      "\n",
      "Chi-Square Statistic for executive_member vs three_diversity: 5.322495523790686\n",
      "P-Value: 0.06986099744780905\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for direct_service vs three_diversity:\n",
      "three_diversity   0    1   2\n",
      "direct_service              \n",
      "0                35  100  90\n",
      "1                 3    8   9\n",
      "\n",
      "Row-normalized proportions for direct_service vs three_diversity:\n",
      "three_diversity         0         1     2\n",
      "direct_service                           \n",
      "0                0.155556  0.444444  0.40\n",
      "1                0.150000  0.400000  0.45\n",
      "\n",
      "Chi-Square Statistic for direct_service vs three_diversity: 0.19959586114556843\n",
      "P-Value: 0.9050202764891747\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_behavior_analysis vs three_diversity:\n",
      "three_diversity         0   1   2\n",
      "ceo_behavior_analysis            \n",
      "0                      20  50  45\n",
      "1                      18  58  54\n",
      "\n",
      "Row-normalized proportions for ceo_behavior_analysis vs three_diversity:\n",
      "three_diversity               0         1         2\n",
      "ceo_behavior_analysis                              \n",
      "0                      0.173913  0.434783  0.391304\n",
      "1                      0.138462  0.446154  0.415385\n",
      "\n",
      "Chi-Square Statistic for ceo_behavior_analysis vs three_diversity: 0.5999189809258462\n",
      "P-Value: 0.7408482314927538\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_business vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "ceo_business               \n",
      "0                19  76  69\n",
      "1                19  32  30\n",
      "\n",
      "Row-normalized proportions for ceo_business vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "ceo_business                                 \n",
      "0                0.115854  0.463415  0.420732\n",
      "1                0.234568  0.395062  0.370370\n",
      "\n",
      "Chi-Square Statistic for ceo_business vs three_diversity: 5.84163234776755\n",
      "P-Value: 0.053889685992127734\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_psychology vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "ceo_psychology             \n",
      "0                32  87  80\n",
      "1                 6  21  19\n",
      "\n",
      "Row-normalized proportions for ceo_psychology vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "ceo_psychology                               \n",
      "0                0.160804  0.437186  0.402010\n",
      "1                0.130435  0.456522  0.413043\n",
      "\n",
      "Chi-Square Statistic for ceo_psychology vs three_diversity: 0.2651205168295769\n",
      "P-Value: 0.8758501551740888\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_education vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "ceo_education              \n",
      "0                31  88  84\n",
      "1                 7  20  15\n",
      "\n",
      "Row-normalized proportions for ceo_education vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "ceo_education                                \n",
      "0                0.152709  0.433498  0.413793\n",
      "1                0.166667  0.476190  0.357143\n",
      "\n",
      "Chi-Square Statistic for ceo_education vs three_diversity: 0.4639839614128795\n",
      "P-Value: 0.7929524821999208\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_other_allied vs three_diversity:\n",
      "three_diversity    0   1   2\n",
      "ceo_other_allied            \n",
      "0                 35  97  90\n",
      "1                  3  11   9\n",
      "\n",
      "Row-normalized proportions for ceo_other_allied vs three_diversity:\n",
      "three_diversity          0         1         2\n",
      "ceo_other_allied                              \n",
      "0                 0.157658  0.436937  0.405405\n",
      "1                 0.130435  0.478261  0.391304\n",
      "\n",
      "Chi-Square Statistic for ceo_other_allied vs three_diversity: 0.1905684461939228\n",
      "P-Value: 0.909114506403856\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for midwest_us vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "midwest_us                 \n",
      "0                21  81  79\n",
      "1                17  27  20\n",
      "\n",
      "Row-normalized proportions for midwest_us vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "midwest_us                                   \n",
      "0                0.116022  0.447514  0.436464\n",
      "1                0.265625  0.421875  0.312500\n",
      "\n",
      "Chi-Square Statistic for midwest_us vs three_diversity: 8.69129174532839\n",
      "P-Value: 0.012963133010136962\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for south_us vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "south_us                   \n",
      "0                22  72  55\n",
      "1                16  36  44\n",
      "\n",
      "Row-normalized proportions for south_us vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "south_us                                     \n",
      "0                0.147651  0.483221  0.369128\n",
      "1                0.166667  0.375000  0.458333\n",
      "\n",
      "Chi-Square Statistic for south_us vs three_diversity: 2.8370504467731594\n",
      "P-Value: 0.2420707540654511\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for west_us vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "west_us                    \n",
      "0                28  90  64\n",
      "1                10  18  35\n",
      "\n",
      "Row-normalized proportions for west_us vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "west_us                                      \n",
      "0                0.153846  0.494505  0.351648\n",
      "1                0.158730  0.285714  0.555556\n",
      "\n",
      "Chi-Square Statistic for west_us vs three_diversity: 9.45090809125897\n",
      "P-Value: 0.008866687115290484\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for northeast_us vs three_diversity:\n",
      "three_diversity   0   1   2\n",
      "northeast_us               \n",
      "0                32  73  76\n",
      "1                 6  35  23\n",
      "\n",
      "Row-normalized proportions for northeast_us vs three_diversity:\n",
      "three_diversity         0         1         2\n",
      "northeast_us                                 \n",
      "0                0.176796  0.403315  0.419890\n",
      "1                0.093750  0.546875  0.359375\n",
      "\n",
      "Chi-Square Statistic for northeast_us vs three_diversity: 4.741415427201404\n",
      "P-Value: 0.09341459210145642\n",
      "Degrees of Freedom: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for other_geo vs three_diversity:\n",
      "three_diversity   0    1   2\n",
      "other_geo                   \n",
      "0                38  101  94\n",
      "1                 0    7   5\n",
      "\n",
      "Row-normalized proportions for other_geo vs three_diversity:\n",
      "three_diversity        0         1         2\n",
      "other_geo                                   \n",
      "0                0.16309  0.433476  0.403433\n",
      "1                0.00000  0.583333  0.416667\n",
      "\n",
      "Chi-Square Statistic for other_geo vs three_diversity: 1.5782269558449815\n",
      "P-Value: 0.4542473171090168\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for org_size vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for degree_ordinal vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for case_manager vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for director vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for executive_member vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for direct_service vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_behavior_analysis vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_business vs three_diversity: 0.8622349758740437\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_psychology vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_education vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_other_allied vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for midwest_us vs three_diversity: 0.2074101281621914\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for south_us vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for west_us vs three_diversity: 0.14186699384464774\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for northeast_us vs three_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for other_geo vs three_diversity: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Target column to compare against other categorical variables with chi-squared analyses\n",
    "target_col = 'three_diversity'\n",
    "\n",
    "# List of other columns to compare \n",
    "columns_to_compare = ['org_size','degree_ordinal', 'case_manager', 'director', 'executive_member', \\\n",
    "                     'direct_service', 'ceo_behavior_analysis', 'ceo_business', 'ceo_psychology', \\\n",
    "                     'ceo_education', 'ceo_other_allied', 'midwest_us', 'south_us', 'west_us', 'northeast_us', \\\n",
    "                     'other_geo']\n",
    "\n",
    "# Store p-values for Bonferroni correction later\n",
    "p_values = []\n",
    "\n",
    "# Loop through columns and perform chi-square tests\n",
    "for col in columns_to_compare:\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(merged_div_df[col], merged_div_df[target_col])\n",
    "    \n",
    "    # Normalize the proportions row-wise\n",
    "    row_normalized = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    \n",
    "    print(f\"\\nContingency Table for {col} vs {target_col}:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    print(f\"\\nRow-normalized proportions for {col} vs {target_col}:\")\n",
    "    print(row_normalized)\n",
    "\n",
    "    # Run the Chi-Square test using statsmodels\n",
    "    chi2_result = sm.stats.Table(contingency_table).test_nominal_association()\n",
    "\n",
    "    # Output the test results\n",
    "    print(f\"\\nChi-Square Statistic for {col} vs {target_col}: {chi2_result.statistic}\")\n",
    "    print(f\"P-Value: {chi2_result.pvalue}\")\n",
    "    print(f\"Degrees of Freedom: {chi2_result.df}\")\n",
    "\n",
    "    # Store p-values for Bonferroni correction\n",
    "    p_values.append(chi2_result.pvalue)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "adjusted_p_values = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Output the adjusted p-values\n",
    "for col, adj_p in zip(columns_to_compare, adjusted_p_values):\n",
    "    print(f\"\\nAdjusted P-Value (Bonferroni) for {col} vs {target_col}: {adj_p}\")\n",
    "    \n",
    "\n",
    "#ceo_business vs. three_diversity - but adjusted p-value is .86\n",
    "#midwest_us vs. three_diversity - but adjusted p-value is .21\n",
    "#west_us vs. three_diversity - bud adjusted p-value is .14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0d991047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for org_size vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "org_size                           \n",
      "1               4  10   9   3  1  0\n",
      "2               6  37  23  13  0  1\n",
      "3              15  23  10   4  1  0\n",
      "4               7  15   9   5  0  0\n",
      "5               1  12   3   2  1  0\n",
      "6               4   6   6   2  0  0\n",
      "7               1   2   1   3  0  0\n",
      "8               0   3   1   1  0  0\n",
      "\n",
      "Row-normalized proportions for org_size vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4       5\n",
      "org_size                                                               \n",
      "1              0.148148  0.370370  0.333333  0.111111  0.037037  0.0000\n",
      "2              0.075000  0.462500  0.287500  0.162500  0.000000  0.0125\n",
      "3              0.283019  0.433962  0.188679  0.075472  0.018868  0.0000\n",
      "4              0.194444  0.416667  0.250000  0.138889  0.000000  0.0000\n",
      "5              0.052632  0.631579  0.157895  0.105263  0.052632  0.0000\n",
      "6              0.222222  0.333333  0.333333  0.111111  0.000000  0.0000\n",
      "7              0.142857  0.285714  0.142857  0.428571  0.000000  0.0000\n",
      "8              0.000000  0.600000  0.200000  0.200000  0.000000  0.0000\n",
      "\n",
      "Chi-Square Statistic for org_size vs sum_diversity: 30.39034297675368\n",
      "P-Value: 0.6902291427659648\n",
      "Degrees of Freedom: 35\n",
      "\n",
      "Contingency Table for degree_ordinal vs sum_diversity:\n",
      "sum_diversity    0   1   2   3  4  5\n",
      "degree_ordinal                      \n",
      "-1               1   1   3   3  1  0\n",
      " 0               0   1   0   0  0  0\n",
      " 1               4   6   3   0  0  1\n",
      " 2              27  75  45  20  2  0\n",
      " 3               6  25  11  10  0  0\n",
      "\n",
      "Row-normalized proportions for degree_ordinal vs sum_diversity:\n",
      "sum_diversity          0         1         2         3         4         5\n",
      "degree_ordinal                                                            \n",
      "-1              0.111111  0.111111  0.333333  0.333333  0.111111  0.000000\n",
      " 0              0.000000  1.000000  0.000000  0.000000  0.000000  0.000000\n",
      " 1              0.285714  0.428571  0.214286  0.000000  0.000000  0.071429\n",
      " 2              0.159763  0.443787  0.266272  0.118343  0.011834  0.000000\n",
      " 3              0.115385  0.480769  0.211538  0.192308  0.000000  0.000000\n",
      "\n",
      "Chi-Square Statistic for degree_ordinal vs sum_diversity: 29.81506047310914\n",
      "P-Value: 0.07290627779410352\n",
      "Degrees of Freedom: 20\n",
      "\n",
      "Contingency Table for case_manager vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "case_manager                       \n",
      "0              28  70  42  20  1  1\n",
      "1              10  38  20  13  2  0\n",
      "\n",
      "Row-normalized proportions for case_manager vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4         5\n",
      "case_manager                                                             \n",
      "0              0.172840  0.432099  0.259259  0.123457  0.006173  0.006173\n",
      "1              0.120482  0.457831  0.240964  0.156627  0.024096  0.000000\n",
      "\n",
      "Chi-Square Statistic for case_manager vs sum_diversity: 3.0055846304854303\n",
      "P-Value: 0.6991247964538078\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for director vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "director                           \n",
      "0              19  56  37  24  3  1\n",
      "1              19  52  25   9  0  0\n",
      "\n",
      "Row-normalized proportions for director vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4         5\n",
      "director                                                                 \n",
      "0              0.135714  0.400000  0.264286  0.171429  0.021429  0.007143\n",
      "1              0.180952  0.495238  0.238095  0.085714  0.000000  0.000000\n",
      "\n",
      "Chi-Square Statistic for director vs sum_diversity: 6.669508090958399\n",
      "P-Value: 0.24640221517680372\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for executive_member vs sum_diversity:\n",
      "sum_diversity      0   1   2   3  4  5\n",
      "executive_member                      \n",
      "0                 29  89  41  24  2  1\n",
      "1                  9  19  21   9  1  0\n",
      "\n",
      "Row-normalized proportions for executive_member vs sum_diversity:\n",
      "sum_diversity            0         1         2         3         4         5\n",
      "executive_member                                                            \n",
      "0                 0.155914  0.478495  0.220430  0.129032  0.010753  0.005376\n",
      "1                 0.152542  0.322034  0.355932  0.152542  0.016949  0.000000\n",
      "\n",
      "Chi-Square Statistic for executive_member vs sum_diversity: 6.1051740384308975\n",
      "P-Value: 0.2961191623504198\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for direct_service vs sum_diversity:\n",
      "sum_diversity    0    1   2   3  4  5\n",
      "direct_service                       \n",
      "0               35  100  58  30  2  0\n",
      "1                3    8   4   3  1  1\n",
      "\n",
      "Row-normalized proportions for direct_service vs sum_diversity:\n",
      "sum_diversity          0         1         2         3         4     5\n",
      "direct_service                                                        \n",
      "0               0.155556  0.444444  0.257778  0.133333  0.008889  0.00\n",
      "1               0.150000  0.400000  0.200000  0.150000  0.050000  0.05\n",
      "\n",
      "Chi-Square Statistic for direct_service vs sum_diversity: 9.76750341701935\n",
      "P-Value: 0.08209742918841434\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_behavior_analysis vs sum_diversity:\n",
      "sum_diversity           0   1   2   3  4  5\n",
      "ceo_behavior_analysis                      \n",
      "0                      20  50  30  13  1  1\n",
      "1                      18  58  32  20  2  0\n",
      "\n",
      "Row-normalized proportions for ceo_behavior_analysis vs sum_diversity:\n",
      "sum_diversity                 0         1         2         3         4  \\\n",
      "ceo_behavior_analysis                                                     \n",
      "0                      0.173913  0.434783  0.260870  0.113043  0.008696   \n",
      "1                      0.138462  0.446154  0.246154  0.153846  0.015385   \n",
      "\n",
      "sum_diversity                 5  \n",
      "ceo_behavior_analysis            \n",
      "0                      0.008696  \n",
      "1                      0.000000  \n",
      "\n",
      "Chi-Square Statistic for ceo_behavior_analysis vs sum_diversity: 1.775683544320421\n",
      "P-Value: 0.8792303430937874\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_business vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "ceo_business                       \n",
      "0              19  76  39  28  2  0\n",
      "1              19  32  23   5  1  1\n",
      "\n",
      "Row-normalized proportions for ceo_business vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4         5\n",
      "ceo_business                                                             \n",
      "0              0.115854  0.463415  0.237805  0.170732  0.012195  0.000000\n",
      "1              0.234568  0.395062  0.283951  0.061728  0.012346  0.012346\n",
      "\n",
      "Chi-Square Statistic for ceo_business vs sum_diversity: 11.517429635272828\n",
      "P-Value: 0.042033062952875344\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_psychology vs sum_diversity:\n",
      "sum_diversity    0   1   2   3  4  5\n",
      "ceo_psychology                      \n",
      "0               32  87  52  25  2  1\n",
      "1                6  21  10   8  1  0\n",
      "\n",
      "Row-normalized proportions for ceo_psychology vs sum_diversity:\n",
      "sum_diversity          0         1         2         3         4         5\n",
      "ceo_psychology                                                            \n",
      "0               0.160804  0.437186  0.261307  0.125628  0.010050  0.005025\n",
      "1               0.130435  0.456522  0.217391  0.173913  0.021739  0.000000\n",
      "\n",
      "Chi-Square Statistic for ceo_psychology vs sum_diversity: 1.7941579546333353\n",
      "P-Value: 0.8768304328746822\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_education vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "ceo_education                      \n",
      "0              31  88  51  29  3  1\n",
      "1               7  20  11   4  0  0\n",
      "\n",
      "Row-normalized proportions for ceo_education vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4         5\n",
      "ceo_education                                                            \n",
      "0              0.152709  0.433498  0.251232  0.142857  0.014778  0.004926\n",
      "1              0.166667  0.476190  0.261905  0.095238  0.000000  0.000000\n",
      "\n",
      "Chi-Square Statistic for ceo_education vs sum_diversity: 1.0501307820246057\n",
      "P-Value: 0.9584218871499014\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_other_allied vs sum_diversity:\n",
      "sum_diversity      0   1   2   3  4  5\n",
      "ceo_other_allied                      \n",
      "0                 35  97  55  31  3  1\n",
      "1                  3  11   7   2  0  0\n",
      "\n",
      "Row-normalized proportions for ceo_other_allied vs sum_diversity:\n",
      "sum_diversity            0         1         2         3         4         5\n",
      "ceo_other_allied                                                            \n",
      "0                 0.157658  0.436937  0.247748  0.139640  0.013514  0.004505\n",
      "1                 0.130435  0.478261  0.304348  0.086957  0.000000  0.000000\n",
      "\n",
      "Chi-Square Statistic for ceo_other_allied vs sum_diversity: 1.8783925716226229\n",
      "P-Value: 0.865702877628158\n",
      "Degrees of Freedom: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for midwest_us vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "midwest_us                         \n",
      "0              21  81  47  28  3  1\n",
      "1              17  27  15   5  0  0\n",
      "\n",
      "Row-normalized proportions for midwest_us vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4         5\n",
      "midwest_us                                                               \n",
      "0              0.116022  0.447514  0.259669  0.154696  0.016575  0.005525\n",
      "1              0.265625  0.421875  0.234375  0.078125  0.000000  0.000000\n",
      "\n",
      "Chi-Square Statistic for midwest_us vs sum_diversity: 9.285307893431035\n",
      "P-Value: 0.09821118531360296\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for south_us vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "south_us                           \n",
      "0              22  72  35  18  2  0\n",
      "1              16  36  27  15  1  1\n",
      "\n",
      "Row-normalized proportions for south_us vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4         5\n",
      "south_us                                                                 \n",
      "0              0.147651  0.483221  0.234899  0.120805  0.013423  0.000000\n",
      "1              0.166667  0.375000  0.281250  0.156250  0.010417  0.010417\n",
      "\n",
      "Chi-Square Statistic for south_us vs sum_diversity: 3.247729830353637\n",
      "P-Value: 0.6618532276094542\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for west_us vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "west_us                            \n",
      "0              28  90  39  22  2  1\n",
      "1              10  18  23  11  1  0\n",
      "\n",
      "Row-normalized proportions for west_us vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4         5\n",
      "west_us                                                                  \n",
      "0              0.153846  0.494505  0.214286  0.120879  0.010989  0.005495\n",
      "1              0.158730  0.285714  0.365079  0.174603  0.015873  0.000000\n",
      "\n",
      "Chi-Square Statistic for west_us vs sum_diversity: 9.939142020611012\n",
      "P-Value: 0.07697810968823637\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for northeast_us vs sum_diversity:\n",
      "sum_diversity   0   1   2   3  4  5\n",
      "northeast_us                       \n",
      "0              32  73  48  25  2  1\n",
      "1               6  35  14   8  1  0\n",
      "\n",
      "Row-normalized proportions for northeast_us vs sum_diversity:\n",
      "sum_diversity         0         1         2         3         4         5\n",
      "northeast_us                                                             \n",
      "0              0.176796  0.403315  0.265193  0.138122  0.011050  0.005525\n",
      "1              0.093750  0.546875  0.218750  0.125000  0.015625  0.000000\n",
      "\n",
      "Chi-Square Statistic for northeast_us vs sum_diversity: 4.8766361234249676\n",
      "P-Value: 0.43112060680972264\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for other_geo vs sum_diversity:\n",
      "sum_diversity   0    1   2   3  4  5\n",
      "other_geo                           \n",
      "0              38  101  59  31  3  1\n",
      "1               0    7   3   2  0  0\n",
      "\n",
      "Row-normalized proportions for other_geo vs sum_diversity:\n",
      "sum_diversity        0         1         2         3         4         5\n",
      "other_geo                                                               \n",
      "0              0.16309  0.433476  0.253219  0.133047  0.012876  0.004292\n",
      "1              0.00000  0.583333  0.250000  0.166667  0.000000  0.000000\n",
      "\n",
      "Chi-Square Statistic for other_geo vs sum_diversity: 4.352453708880068\n",
      "P-Value: 0.49986386308737163\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for org_size vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for degree_ordinal vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for case_manager vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for director vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for executive_member vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for direct_service vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_behavior_analysis vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_business vs sum_diversity: 0.6725290072460055\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_psychology vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_education vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_other_allied vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for midwest_us vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for south_us vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for west_us vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for northeast_us vs sum_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for other_geo vs sum_diversity: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Target column to compare against other categorical variables\n",
    "target_col = 'sum_diversity'\n",
    "\n",
    "# List of other columns to compare\n",
    "columns_to_compare = ['org_size','degree_ordinal', 'case_manager', 'director', 'executive_member', \\\n",
    "                     'direct_service', 'ceo_behavior_analysis', 'ceo_business', 'ceo_psychology', \\\n",
    "                     'ceo_education', 'ceo_other_allied', 'midwest_us', 'south_us', 'west_us', 'northeast_us', \\\n",
    "                     'other_geo']\n",
    "\n",
    "# Store p-values for Bonferroni correction later\n",
    "p_values = []\n",
    "\n",
    "# Loop through columns and perform chi-square tests\n",
    "for col in columns_to_compare:\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(merged_div_df[col], merged_div_df[target_col])\n",
    "    \n",
    "    # Normalize the proportions row-wise\n",
    "    row_normalized = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    \n",
    "    print(f\"\\nContingency Table for {col} vs {target_col}:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    print(f\"\\nRow-normalized proportions for {col} vs {target_col}:\")\n",
    "    print(row_normalized)\n",
    "\n",
    "    # Run the Chi-Square test using statsmodels\n",
    "    chi2_result = sm.stats.Table(contingency_table).test_nominal_association()\n",
    "\n",
    "    # Output the test results\n",
    "    print(f\"\\nChi-Square Statistic for {col} vs {target_col}: {chi2_result.statistic}\")\n",
    "    print(f\"P-Value: {chi2_result.pvalue}\")\n",
    "    print(f\"Degrees of Freedom: {chi2_result.df}\")\n",
    "\n",
    "    # Store p-values for Bonferroni correction\n",
    "    p_values.append(chi2_result.pvalue)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "adjusted_p_values = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Output the adjusted p-values\n",
    "for col, adj_p in zip(columns_to_compare, adjusted_p_values):\n",
    "    print(f\"\\nAdjusted P-Value (Bonferroni) for {col} vs {target_col}: {adj_p}\")\n",
    "\n",
    "###???????????????????????????????????????? keep (here or somewhere else????)\n",
    "#ceo_business vs sum_diversity - but adjusted p-value is .67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a1681adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for org_size vs binary_diversity:\n",
      "binary_diversity   0   1\n",
      "org_size                \n",
      "1                  4  23\n",
      "2                  6  74\n",
      "3                 15  38\n",
      "4                  7  29\n",
      "5                  1  18\n",
      "6                  4  14\n",
      "7                  1   6\n",
      "8                  0   5\n",
      "\n",
      "Row-normalized proportions for org_size vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "org_size                            \n",
      "1                 0.148148  0.851852\n",
      "2                 0.075000  0.925000\n",
      "3                 0.283019  0.716981\n",
      "4                 0.194444  0.805556\n",
      "5                 0.052632  0.947368\n",
      "6                 0.222222  0.777778\n",
      "7                 0.142857  0.857143\n",
      "8                 0.000000  1.000000\n",
      "\n",
      "Chi-Square Statistic for org_size vs binary_diversity: 13.167574317729896\n",
      "P-Value: 0.06812921885538858\n",
      "Degrees of Freedom: 7\n",
      "\n",
      "Contingency Table for degree_ordinal vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "degree_ordinal           \n",
      "-1                 1    8\n",
      " 0                 0    1\n",
      " 1                 4   10\n",
      " 2                27  142\n",
      " 3                 6   46\n",
      "\n",
      "Row-normalized proportions for degree_ordinal vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "degree_ordinal                      \n",
      "-1                0.111111  0.888889\n",
      " 0                0.000000  1.000000\n",
      " 1                0.285714  0.714286\n",
      " 2                0.159763  0.840237\n",
      " 3                0.115385  0.884615\n",
      "\n",
      "Chi-Square Statistic for degree_ordinal vs binary_diversity: 2.9409028826929764\n",
      "P-Value: 0.5677633765550406\n",
      "Degrees of Freedom: 4\n",
      "\n",
      "Contingency Table for case_manager vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "case_manager             \n",
      "0                 28  134\n",
      "1                 10   73\n",
      "\n",
      "Row-normalized proportions for case_manager vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "case_manager                        \n",
      "0                 0.172840  0.827160\n",
      "1                 0.120482  0.879518\n",
      "\n",
      "Chi-Square Statistic for case_manager vs binary_diversity: 1.1480593863622035\n",
      "P-Value: 0.2839557567365083\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for director vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "director                 \n",
      "0                 19  121\n",
      "1                 19   86\n",
      "\n",
      "Row-normalized proportions for director vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "director                            \n",
      "0                 0.135714  0.864286\n",
      "1                 0.180952  0.819048\n",
      "\n",
      "Chi-Square Statistic for director vs binary_diversity: 0.9369967793880837\n",
      "P-Value: 0.3330513917925114\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for executive_member vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "executive_member         \n",
      "0                 29  157\n",
      "1                  9   50\n",
      "\n",
      "Row-normalized proportions for executive_member vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "executive_member                    \n",
      "0                 0.155914  0.844086\n",
      "1                 0.152542  0.847458\n",
      "\n",
      "Chi-Square Statistic for executive_member vs binary_diversity: 0.003885533293194992\n",
      "P-Value: 0.9502968163798595\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for direct_service vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "direct_service           \n",
      "0                 35  190\n",
      "1                  3   17\n",
      "\n",
      "Row-normalized proportions for direct_service vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "direct_service                      \n",
      "0                 0.155556  0.844444\n",
      "1                 0.150000  0.850000\n",
      "\n",
      "Chi-Square Statistic for direct_service vs binary_diversity: 0.004325931576122309\n",
      "P-Value: 0.9475594964418196\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_behavior_analysis vs binary_diversity:\n",
      "binary_diversity        0    1\n",
      "ceo_behavior_analysis         \n",
      "0                      20   95\n",
      "1                      18  112\n",
      "\n",
      "Row-normalized proportions for ceo_behavior_analysis vs binary_diversity:\n",
      "binary_diversity              0         1\n",
      "ceo_behavior_analysis                    \n",
      "0                      0.173913  0.826087\n",
      "1                      0.138462  0.861538\n",
      "\n",
      "Chi-Square Statistic for ceo_behavior_analysis vs binary_diversity: 0.585224755456573\n",
      "P-Value: 0.444271639800046\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_business vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "ceo_business             \n",
      "0                 19  145\n",
      "1                 19   62\n",
      "\n",
      "Row-normalized proportions for ceo_business vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "ceo_business                        \n",
      "0                 0.115854  0.884146\n",
      "1                 0.234568  0.765432\n",
      "\n",
      "Chi-Square Statistic for ceo_business vs binary_diversity: 5.831048611747523\n",
      "P-Value: 0.015745735163680386\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_psychology vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "ceo_psychology           \n",
      "0                 32  167\n",
      "1                  6   40\n",
      "\n",
      "Row-normalized proportions for ceo_psychology vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "ceo_psychology                      \n",
      "0                 0.160804  0.839196\n",
      "1                 0.130435  0.869565\n",
      "\n",
      "Chi-Square Statistic for ceo_psychology vs binary_diversity: 0.262960687206581\n",
      "P-Value: 0.6080933803645738\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_education vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "ceo_education            \n",
      "0                 31  172\n",
      "1                  7   35\n",
      "\n",
      "Row-normalized proportions for ceo_education vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "ceo_education                       \n",
      "0                 0.152709  0.847291\n",
      "1                 0.166667  0.833333\n",
      "\n",
      "Chi-Square Statistic for ceo_education vs binary_diversity: 0.051732174848248554\n",
      "P-Value: 0.8200761137643442\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_other_allied vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "ceo_other_allied         \n",
      "0                 35  187\n",
      "1                  3   20\n",
      "\n",
      "Row-normalized proportions for ceo_other_allied vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "ceo_other_allied                    \n",
      "0                 0.157658  0.842342\n",
      "1                 0.130435  0.869565\n",
      "\n",
      "Chi-Square Statistic for ceo_other_allied vs binary_diversity: 0.1178585062029497\n",
      "P-Value: 0.7313688190033885\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for midwest_us vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "midwest_us               \n",
      "0                 21  160\n",
      "1                 17   47\n",
      "\n",
      "Row-normalized proportions for midwest_us vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "midwest_us                          \n",
      "0                 0.116022  0.883978\n",
      "1                 0.265625  0.734375\n",
      "\n",
      "Chi-Square Statistic for midwest_us vs binary_diversity: 8.075152241428594\n",
      "P-Value: 0.004487633688397663\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for south_us vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "south_us                 \n",
      "0                 22  127\n",
      "1                 16   80\n",
      "\n",
      "Row-normalized proportions for south_us vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "south_us                            \n",
      "0                 0.147651  0.852349\n",
      "1                 0.166667  0.833333\n",
      "\n",
      "Chi-Square Statistic for south_us vs binary_diversity: 0.16109885321870648\n",
      "P-Value: 0.688146838031179\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for west_us vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "west_us                  \n",
      "0                 28  154\n",
      "1                 10   53\n",
      "\n",
      "Row-normalized proportions for west_us vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "west_us                             \n",
      "0                 0.153846  0.846154\n",
      "1                 0.158730  0.841270\n",
      "\n",
      "Chi-Square Statistic for west_us vs binary_diversity: 0.008518757565286928\n",
      "P-Value: 0.9264619799333581\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for northeast_us vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "northeast_us             \n",
      "0                 32  149\n",
      "1                  6   58\n",
      "\n",
      "Row-normalized proportions for northeast_us vs binary_diversity:\n",
      "binary_diversity         0         1\n",
      "northeast_us                        \n",
      "0                 0.176796  0.823204\n",
      "1                 0.093750  0.906250\n",
      "\n",
      "Chi-Square Statistic for northeast_us vs binary_diversity: 2.488305717803598\n",
      "P-Value: 0.11469513657364505\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for other_geo vs binary_diversity:\n",
      "binary_diversity   0    1\n",
      "other_geo                \n",
      "0                 38  195\n",
      "1                  0   12\n",
      "\n",
      "Row-normalized proportions for other_geo vs binary_diversity:\n",
      "binary_diversity        0        1\n",
      "other_geo                         \n",
      "0                 0.16309  0.83691\n",
      "1                 0.00000  1.00000\n",
      "\n",
      "Chi-Square Statistic for other_geo vs binary_diversity: 1.3593547449005563\n",
      "P-Value: 0.24364934122371495\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for org_size vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for degree_ordinal vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for case_manager vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for director vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for executive_member vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for direct_service vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_behavior_analysis vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_business vs binary_diversity: 0.25193176261888617\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_psychology vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_education vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_other_allied vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for midwest_us vs binary_diversity: 0.07180213901436261\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for south_us vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for west_us vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for northeast_us vs binary_diversity: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for other_geo vs binary_diversity: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Target column to compare against other categorical variables with chi-squared analyses\n",
    "target_col = 'binary_diversity'\n",
    "\n",
    "# List of other columns to compare\n",
    "columns_to_compare = ['org_size','degree_ordinal', 'case_manager', 'director', 'executive_member', \\\n",
    "                     'direct_service', 'ceo_behavior_analysis', 'ceo_business', 'ceo_psychology', \\\n",
    "                     'ceo_education', 'ceo_other_allied', 'midwest_us', 'south_us', 'west_us', 'northeast_us', \\\n",
    "                     'other_geo']\n",
    "\n",
    "# Store p-values for Bonferroni correction later\n",
    "p_values = []\n",
    "\n",
    "# Loop through columns and perform chi-square tests\n",
    "for col in columns_to_compare:\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(merged_div_df[col], merged_div_df[target_col])\n",
    "    \n",
    "    # Normalize the proportions row-wise\n",
    "    row_normalized = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    \n",
    "    print(f\"\\nContingency Table for {col} vs {target_col}:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    print(f\"\\nRow-normalized proportions for {col} vs {target_col}:\")\n",
    "    print(row_normalized)\n",
    "\n",
    "    # Run the Chi-Square test using statsmodels\n",
    "    chi2_result = sm.stats.Table(contingency_table).test_nominal_association()\n",
    "\n",
    "    # Output the test results\n",
    "    print(f\"\\nChi-Square Statistic for {col} vs {target_col}: {chi2_result.statistic}\")\n",
    "    print(f\"P-Value: {chi2_result.pvalue}\")\n",
    "    print(f\"Degrees of Freedom: {chi2_result.df}\")\n",
    "\n",
    "    # Store p-values for Bonferroni correction\n",
    "    p_values.append(chi2_result.pvalue)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "adjusted_p_values = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Output the adjusted p-values\n",
    "for col, adj_p in zip(columns_to_compare, adjusted_p_values):\n",
    "    print(f\"\\nAdjusted P-Value (Bonferroni) for {col} vs {target_col}: {adj_p}\")\n",
    "\n",
    "##?????????????????????????????????????????????? keep (here or somewhere else??)\n",
    "#ceo_business and binary_diversity - but adjusted p-value is .25\n",
    "#midwest_us and binary_diversity - bud adjusted p-value is .07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0bf903b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for org_size vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0   1\n",
      "org_size                           \n",
      "1                             1  26\n",
      "2                             1  79\n",
      "3                             1  52\n",
      "4                             0  36\n",
      "5                             0  19\n",
      "6                             2  16\n",
      "7                             0   7\n",
      "8                             0   5\n",
      "\n",
      "Row-normalized proportions for org_size vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "org_size                                        \n",
      "1                             0.037037  0.962963\n",
      "2                             0.012500  0.987500\n",
      "3                             0.018868  0.981132\n",
      "4                             0.000000  1.000000\n",
      "5                             0.000000  1.000000\n",
      "6                             0.111111  0.888889\n",
      "7                             0.000000  1.000000\n",
      "8                             0.000000  1.000000\n",
      "\n",
      "Chi-Square Statistic for org_size vs binary_diversity_with_female: 6.925404810919869\n",
      "P-Value: 0.43668785027471757\n",
      "Degrees of Freedom: 7\n",
      "\n",
      "Contingency Table for degree_ordinal vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "degree_ordinal                      \n",
      "-1                            0    9\n",
      " 0                            0    1\n",
      " 1                            0   14\n",
      " 2                            4  165\n",
      " 3                            1   51\n",
      "\n",
      "Row-normalized proportions for degree_ordinal vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "degree_ordinal                                  \n",
      "-1                            0.000000  1.000000\n",
      " 0                            0.000000  1.000000\n",
      " 1                            0.000000  1.000000\n",
      " 2                            0.023669  0.976331\n",
      " 3                            0.019231  0.980769\n",
      "\n",
      "Chi-Square Statistic for degree_ordinal vs binary_diversity_with_female: 5.948841093949219\n",
      "P-Value: 0.20300156637432543\n",
      "Degrees of Freedom: 4\n",
      "\n",
      "Contingency Table for case_manager vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "case_manager                        \n",
      "0                             4  158\n",
      "1                             1   82\n",
      "\n",
      "Row-normalized proportions for case_manager vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "case_manager                                    \n",
      "0                             0.024691  0.975309\n",
      "1                             0.012048  0.987952\n",
      "\n",
      "Chi-Square Statistic for case_manager vs binary_diversity_with_female: 0.4388231940106102\n",
      "P-Value: 0.507690997021848\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for director vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "director                            \n",
      "0                             1  139\n",
      "1                             4  101\n",
      "\n",
      "Row-normalized proportions for director vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "director                                        \n",
      "0                             0.007143  0.992857\n",
      "1                             0.038095  0.961905\n",
      "\n",
      "Chi-Square Statistic for director vs binary_diversity_with_female: 2.8753472222222234\n",
      "P-Value: 0.0899455729014289\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for executive_member vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "executive_member                    \n",
      "0                             3  183\n",
      "1                             2   57\n",
      "\n",
      "Row-normalized proportions for executive_member vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "executive_member                                \n",
      "0                             0.016129  0.983871\n",
      "1                             0.033898  0.966102\n",
      "\n",
      "Chi-Square Statistic for executive_member vs binary_diversity_with_female: 0.7074391744122474\n",
      "P-Value: 0.4002952569671986\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for direct_service vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "direct_service                      \n",
      "0                             4  221\n",
      "1                             1   19\n",
      "\n",
      "Row-normalized proportions for direct_service vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "direct_service                                  \n",
      "0                             0.017778  0.982222\n",
      "1                             0.050000  0.950000\n",
      "\n",
      "Chi-Square Statistic for direct_service vs binary_diversity_with_female: 0.9539120370370372\n",
      "P-Value: 0.32872551696860086\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_behavior_analysis vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "ceo_behavior_analysis               \n",
      "0                             3  112\n",
      "1                             2  128\n",
      "\n",
      "Row-normalized proportions for ceo_behavior_analysis vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "ceo_behavior_analysis                           \n",
      "0                             0.026087  0.973913\n",
      "1                             0.015385  0.984615\n",
      "\n",
      "Chi-Square Statistic for ceo_behavior_analysis vs binary_diversity_with_female: 0.3496098104793759\n",
      "P-Value: 0.5543340900235327\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_business vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "ceo_business                        \n",
      "0                             2  162\n",
      "1                             3   78\n",
      "\n",
      "Row-normalized proportions for ceo_business vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "ceo_business                                    \n",
      "0                             0.012195  0.987805\n",
      "1                             0.037037  0.962963\n",
      "\n",
      "Chi-Square Statistic for ceo_business vs binary_diversity_with_female: 1.6737240289069557\n",
      "P-Value: 0.1957604730733441\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_psychology vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "ceo_psychology                      \n",
      "0                             4  195\n",
      "1                             1   45\n",
      "\n",
      "Row-normalized proportions for ceo_psychology vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "ceo_psychology                                  \n",
      "0                             0.020101  0.979899\n",
      "1                             0.021739  0.978261\n",
      "\n",
      "Chi-Square Statistic for ceo_psychology vs binary_diversity_with_female: 0.005018298011798123\n",
      "P-Value: 0.9435251390056357\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_education vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "ceo_education                       \n",
      "0                             3  200\n",
      "1                             2   40\n",
      "\n",
      "Row-normalized proportions for ceo_education vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "ceo_education                                   \n",
      "0                             0.014778  0.985222\n",
      "1                             0.047619  0.952381\n",
      "\n",
      "Chi-Square Statistic for ceo_education vs binary_diversity_with_female: 1.8773946360153262\n",
      "P-Value: 0.17063056003246924\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for ceo_other_allied vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "ceo_other_allied                    \n",
      "0                             4  218\n",
      "1                             1   22\n",
      "\n",
      "Row-normalized proportions for ceo_other_allied vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "ceo_other_allied                                \n",
      "0                             0.018018  0.981982\n",
      "1                             0.043478  0.956522\n",
      "\n",
      "Chi-Square Statistic for ceo_other_allied vs binary_diversity_with_female: 0.6757572790181487\n",
      "P-Value: 0.41105152954691426\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for midwest_us vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "midwest_us                          \n",
      "0                             1  180\n",
      "1                             4   60\n",
      "\n",
      "Row-normalized proportions for midwest_us vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "midwest_us                                      \n",
      "0                             0.005525  0.994475\n",
      "1                             0.062500  0.937500\n",
      "\n",
      "Chi-Square Statistic for midwest_us vs binary_diversity_with_female: 7.6773998618784525\n",
      "P-Value: 0.005591667886938545\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for south_us vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "south_us                            \n",
      "0                             3  146\n",
      "1                             2   94\n",
      "\n",
      "Row-normalized proportions for south_us vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "south_us                                        \n",
      "0                             0.020134  0.979866\n",
      "1                             0.020833  0.979167\n",
      "\n",
      "Chi-Square Statistic for south_us vs binary_diversity_with_female: 0.0014273396718866454\n",
      "P-Value: 0.9698629717328048\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for west_us vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "west_us                             \n",
      "0                             2  180\n",
      "1                             3   60\n",
      "\n",
      "Row-normalized proportions for west_us vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "west_us                                         \n",
      "0                             0.010989  0.989011\n",
      "1                             0.047619  0.952381\n",
      "\n",
      "Chi-Square Statistic for west_us vs binary_diversity_with_female: 3.141025641025641\n",
      "P-Value: 0.07634578456124119\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for northeast_us vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "northeast_us                        \n",
      "0                             5  176\n",
      "1                             0   64\n",
      "\n",
      "Row-normalized proportions for northeast_us vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "northeast_us                                    \n",
      "0                             0.027624  0.972376\n",
      "1                             0.000000  1.000000\n",
      "\n",
      "Chi-Square Statistic for northeast_us vs binary_diversity_with_female: 0.8574627165915871\n",
      "P-Value: 0.35444970771142836\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Contingency Table for other_geo vs binary_diversity_with_female:\n",
      "binary_diversity_with_female  0    1\n",
      "other_geo                           \n",
      "0                             5  228\n",
      "1                             0   12\n",
      "\n",
      "Row-normalized proportions for other_geo vs binary_diversity_with_female:\n",
      "binary_diversity_with_female         0         1\n",
      "other_geo                                       \n",
      "0                             0.021459  0.978541\n",
      "1                             0.000000  1.000000\n",
      "\n",
      "Chi-Square Statistic for other_geo vs binary_diversity_with_female: 0.1862083495903238\n",
      "P-Value: 0.6660912913004826\n",
      "Degrees of Freedom: 1\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for org_size vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for degree_ordinal vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for case_manager vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for director vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for executive_member vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for direct_service vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_behavior_analysis vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_business vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_psychology vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_education vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_other_allied vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for midwest_us vs binary_diversity_with_female: 0.08946668619101672\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for south_us vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for west_us vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for northeast_us vs binary_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for other_geo vs binary_diversity_with_female: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Target column to compare against other categorical variables with chi-squared analyses\n",
    "target_col = 'binary_diversity_with_female'\n",
    "\n",
    "# List of other columns to compare\n",
    "columns_to_compare = ['org_size','degree_ordinal', 'case_manager', 'director', 'executive_member', \\\n",
    "                     'direct_service', 'ceo_behavior_analysis', 'ceo_business', 'ceo_psychology', \\\n",
    "                     'ceo_education', 'ceo_other_allied', 'midwest_us', 'south_us', 'west_us', 'northeast_us', \\\n",
    "                     'other_geo']\n",
    "\n",
    "# Store p-values for Bonferroni correction later\n",
    "p_values = []\n",
    "\n",
    "# Loop through columns and perform chi-square tests\n",
    "for col in columns_to_compare:\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(merged_div_df[col], merged_div_df[target_col])\n",
    "    \n",
    "    # Normalize the proportions row-wise\n",
    "    row_normalized = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    \n",
    "    print(f\"\\nContingency Table for {col} vs {target_col}:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    print(f\"\\nRow-normalized proportions for {col} vs {target_col}:\")\n",
    "    print(row_normalized)\n",
    "\n",
    "    # Run the Chi-Square test using statsmodels\n",
    "    chi2_result = sm.stats.Table(contingency_table).test_nominal_association()\n",
    "\n",
    "    # Output the test results\n",
    "    print(f\"\\nChi-Square Statistic for {col} vs {target_col}: {chi2_result.statistic}\")\n",
    "    print(f\"P-Value: {chi2_result.pvalue}\")\n",
    "    print(f\"Degrees of Freedom: {chi2_result.df}\")\n",
    "\n",
    "    # Store p-values for Bonferroni correction\n",
    "    p_values.append(chi2_result.pvalue)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "adjusted_p_values = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Output the adjusted p-values\n",
    "for col, adj_p in zip(columns_to_compare, adjusted_p_values):\n",
    "    print(f\"\\nAdjusted P-Value (Bonferroni) for {col} vs {target_col}: {adj_p}\")\n",
    "    \n",
    "####????????????????????????????????????????????? keep (here or somewhere else??)\n",
    "#midwest_us vs binary_diversity_with_female - but adjusted p-value is .09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7b80c58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for org_size vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3  4  5\n",
      "org_size                                      \n",
      "1                          1   6   7  10  2  1\n",
      "2                          1   9  36  24  9  1\n",
      "3                          1  15  25   8  3  1\n",
      "4                          0   8  16   8  4  0\n",
      "5                          0   1  12   4  1  1\n",
      "6                          2   4   5   5  2  0\n",
      "7                          0   3   0   2  2  0\n",
      "8                          0   0   3   1  1  0\n",
      "\n",
      "Row-normalized proportions for org_size vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "org_size                                                                      \n",
      "1                          0.037037  0.222222  0.259259  0.370370  0.074074   \n",
      "2                          0.012500  0.112500  0.450000  0.300000  0.112500   \n",
      "3                          0.018868  0.283019  0.471698  0.150943  0.056604   \n",
      "4                          0.000000  0.222222  0.444444  0.222222  0.111111   \n",
      "5                          0.000000  0.052632  0.631579  0.210526  0.052632   \n",
      "6                          0.111111  0.222222  0.277778  0.277778  0.111111   \n",
      "7                          0.000000  0.428571  0.000000  0.285714  0.285714   \n",
      "8                          0.000000  0.000000  0.600000  0.200000  0.200000   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "org_size                             \n",
      "1                          0.037037  \n",
      "2                          0.012500  \n",
      "3                          0.018868  \n",
      "4                          0.000000  \n",
      "5                          0.052632  \n",
      "6                          0.000000  \n",
      "7                          0.000000  \n",
      "8                          0.000000  \n",
      "\n",
      "Chi-Square Statistic for org_size vs sum_diversity_with_female: 34.39271725664191\n",
      "P-Value: 0.4972491795648285\n",
      "Degrees of Freedom: 35\n",
      "\n",
      "Contingency Table for degree_ordinal vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "degree_ordinal                                 \n",
      "-1                         0   1   1   4   2  1\n",
      " 0                         0   0   1   0   0  0\n",
      " 1                         0   5   5   3   0  1\n",
      " 2                         4  30  73  44  16  2\n",
      " 3                         1  10  24  11   6  0\n",
      "\n",
      "Row-normalized proportions for degree_ordinal vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "degree_ordinal                                                                \n",
      "-1                         0.000000  0.111111  0.111111  0.444444  0.222222   \n",
      " 0                         0.000000  0.000000  1.000000  0.000000  0.000000   \n",
      " 1                         0.000000  0.357143  0.357143  0.214286  0.000000   \n",
      " 2                         0.023669  0.177515  0.431953  0.260355  0.094675   \n",
      " 3                         0.019231  0.192308  0.461538  0.211538  0.115385   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "degree_ordinal                       \n",
      "-1                         0.111111  \n",
      " 0                         0.000000  \n",
      " 1                         0.071429  \n",
      " 2                         0.011834  \n",
      " 3                         0.000000  \n",
      "\n",
      "Chi-Square Statistic for degree_ordinal vs sum_diversity_with_female: 19.786662482148373\n",
      "P-Value: 0.4713441689995703\n",
      "Degrees of Freedom: 20\n",
      "\n",
      "Contingency Table for case_manager vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "case_manager                                   \n",
      "0                          4  31  72  39  14  2\n",
      "1                          1  15  32  23  10  2\n",
      "\n",
      "Row-normalized proportions for case_manager vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "case_manager                                                                  \n",
      "0                          0.024691  0.191358  0.444444  0.240741  0.086420   \n",
      "1                          0.012048  0.180723  0.385542  0.277108  0.120482   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "case_manager                         \n",
      "0                          0.012346  \n",
      "1                          0.024096  \n",
      "\n",
      "Chi-Square Statistic for case_manager vs sum_diversity_with_female: 2.3125007498804706\n",
      "P-Value: 0.8044291135918907\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for director vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "director                                       \n",
      "0                          1  25  55  39  16  4\n",
      "1                          4  21  49  23   8  0\n",
      "\n",
      "Row-normalized proportions for director vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "director                                                                      \n",
      "0                          0.007143  0.178571  0.392857  0.278571  0.114286   \n",
      "1                          0.038095  0.200000  0.466667  0.219048  0.076190   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "director                             \n",
      "0                          0.028571  \n",
      "1                          0.000000  \n",
      "\n",
      "Chi-Square Statistic for director vs sum_diversity_with_female: 7.307953724961301\n",
      "P-Value: 0.1987262130526204\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for executive_member vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "executive_member                               \n",
      "0                          3  36  82  43  19  3\n",
      "1                          2  10  22  19   5  1\n",
      "\n",
      "Row-normalized proportions for executive_member vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "executive_member                                                              \n",
      "0                          0.016129  0.193548  0.440860  0.231183  0.102151   \n",
      "1                          0.033898  0.169492  0.372881  0.322034  0.084746   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "executive_member                     \n",
      "0                          0.016129  \n",
      "1                          0.016949  \n",
      "\n",
      "Chi-Square Statistic for executive_member vs sum_diversity_with_female: 2.919987307442237\n",
      "P-Value: 0.712319996004267\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for direct_service vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "direct_service                                 \n",
      "0                          4  42  98  58  21  2\n",
      "1                          1   4   6   4   3  2\n",
      "\n",
      "Row-normalized proportions for direct_service vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "direct_service                                                                \n",
      "0                          0.017778  0.186667  0.435556  0.257778  0.093333   \n",
      "1                          0.050000  0.200000  0.300000  0.200000  0.150000   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "direct_service                       \n",
      "0                          0.008889  \n",
      "1                          0.100000  \n",
      "\n",
      "Chi-Square Statistic for direct_service vs sum_diversity_with_female: 11.930187347909996\n",
      "P-Value: 0.035756998519986705\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_behavior_analysis vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "ceo_behavior_analysis                          \n",
      "0                          3  24  48  27  11  2\n",
      "1                          2  22  56  35  13  2\n",
      "\n",
      "Row-normalized proportions for ceo_behavior_analysis vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "ceo_behavior_analysis                                                         \n",
      "0                          0.026087  0.208696  0.417391  0.234783  0.095652   \n",
      "1                          0.015385  0.169231  0.430769  0.269231  0.100000   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "ceo_behavior_analysis                \n",
      "0                          0.017391  \n",
      "1                          0.015385  \n",
      "\n",
      "Chi-Square Statistic for ceo_behavior_analysis vs sum_diversity_with_female: 1.1873492265066925\n",
      "P-Value: 0.946085263072837\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_business vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "ceo_business                                   \n",
      "0                          2  26  73  41  20  2\n",
      "1                          3  20  31  21   4  2\n",
      "\n",
      "Row-normalized proportions for ceo_business vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "ceo_business                                                                  \n",
      "0                          0.012195  0.158537  0.445122  0.250000  0.121951   \n",
      "1                          0.037037  0.246914  0.382716  0.259259  0.049383   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "ceo_business                         \n",
      "0                          0.012195  \n",
      "1                          0.024691  \n",
      "\n",
      "Chi-Square Statistic for ceo_business vs sum_diversity_with_female: 7.844345910365169\n",
      "P-Value: 0.16502523861780105\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_psychology vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "ceo_psychology                                 \n",
      "0                          4  40  83  50  19  3\n",
      "1                          1   6  21  12   5  1\n",
      "\n",
      "Row-normalized proportions for ceo_psychology vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "ceo_psychology                                                                \n",
      "0                          0.020101  0.201005  0.417085  0.251256  0.095477   \n",
      "1                          0.021739  0.130435  0.456522  0.260870  0.108696   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "ceo_psychology                       \n",
      "0                          0.015075  \n",
      "1                          0.021739  \n",
      "\n",
      "Chi-Square Statistic for ceo_psychology vs sum_diversity_with_female: 1.3147660462591644\n",
      "P-Value: 0.9334049539009807\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_education vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "ceo_education                                  \n",
      "0                          3  40  83  53  20  4\n",
      "1                          2   6  21   9   4  0\n",
      "\n",
      "Row-normalized proportions for ceo_education vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "ceo_education                                                                 \n",
      "0                          0.014778  0.197044  0.408867  0.261084  0.098522   \n",
      "1                          0.047619  0.142857  0.500000  0.214286  0.095238   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "ceo_education                        \n",
      "0                          0.019704  \n",
      "1                          0.000000  \n",
      "\n",
      "Chi-Square Statistic for ceo_education vs sum_diversity_with_female: 3.4526301728176856\n",
      "P-Value: 0.630567095218134\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for ceo_other_allied vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "ceo_other_allied                               \n",
      "0                          4  41  93  57  23  4\n",
      "1                          1   5  11   5   1  0\n",
      "\n",
      "Row-normalized proportions for ceo_other_allied vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "ceo_other_allied                                                              \n",
      "0                          0.018018  0.184685  0.418919  0.256757  0.103604   \n",
      "1                          0.043478  0.217391  0.478261  0.217391  0.043478   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "ceo_other_allied                     \n",
      "0                          0.018018  \n",
      "1                          0.000000  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi-Square Statistic for ceo_other_allied vs sum_diversity_with_female: 1.8241309643118009\n",
      "P-Value: 0.8729051350152944\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for midwest_us vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "midwest_us                                     \n",
      "0                          1  29  78  48  21  4\n",
      "1                          4  17  26  14   3  0\n",
      "\n",
      "Row-normalized proportions for midwest_us vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "midwest_us                                                                    \n",
      "0                          0.005525  0.160221  0.430939  0.265193  0.116022   \n",
      "1                          0.062500  0.265625  0.406250  0.218750  0.046875   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "midwest_us                           \n",
      "0                          0.022099  \n",
      "1                          0.000000  \n",
      "\n",
      "Chi-Square Statistic for midwest_us vs sum_diversity_with_female: 13.569394473071121\n",
      "P-Value: 0.01858890713338801\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for south_us vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "south_us                                       \n",
      "0                          3  28  68  36  12  2\n",
      "1                          2  18  36  26  12  2\n",
      "\n",
      "Row-normalized proportions for south_us vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "south_us                                                                      \n",
      "0                          0.020134  0.187919  0.456376  0.241611  0.080537   \n",
      "1                          0.020833  0.187500  0.375000  0.270833  0.125000   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "south_us                             \n",
      "0                          0.013423  \n",
      "1                          0.020833  \n",
      "\n",
      "Chi-Square Statistic for south_us vs sum_diversity_with_female: 2.4839036489653146\n",
      "P-Value: 0.7789172385637664\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for west_us vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "west_us                                        \n",
      "0                          2  37  87  36  17  3\n",
      "1                          3   9  17  26   7  1\n",
      "\n",
      "Row-normalized proportions for west_us vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "west_us                                                                       \n",
      "0                          0.010989  0.203297  0.478022  0.197802  0.093407   \n",
      "1                          0.047619  0.142857  0.269841  0.412698  0.111111   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "west_us                              \n",
      "0                          0.016484  \n",
      "1                          0.015873  \n",
      "\n",
      "Chi-Square Statistic for west_us vs sum_diversity_with_female: 17.45681639069545\n",
      "P-Value: 0.003710170856862094\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for northeast_us vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "northeast_us                                   \n",
      "0                          5  36  69  49  19  3\n",
      "1                          0  10  35  13   5  1\n",
      "\n",
      "Row-normalized proportions for northeast_us vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "northeast_us                                                                  \n",
      "0                          0.027624  0.198895  0.381215  0.270718  0.104972   \n",
      "1                          0.000000  0.156250  0.546875  0.203125  0.078125   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "northeast_us                         \n",
      "0                          0.016575  \n",
      "1                          0.015625  \n",
      "\n",
      "Chi-Square Statistic for northeast_us vs sum_diversity_with_female: 5.522170203238926\n",
      "P-Value: 0.35552096172168257\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Contingency Table for other_geo vs sum_diversity_with_female:\n",
      "sum_diversity_with_female  0   1   2   3   4  5\n",
      "other_geo                                      \n",
      "0                          5  45  98  59  22  4\n",
      "1                          0   1   6   3   2  0\n",
      "\n",
      "Row-normalized proportions for other_geo vs sum_diversity_with_female:\n",
      "sum_diversity_with_female         0         1         2         3         4  \\\n",
      "other_geo                                                                     \n",
      "0                          0.021459  0.193133  0.420601  0.253219  0.094421   \n",
      "1                          0.000000  0.083333  0.500000  0.250000  0.166667   \n",
      "\n",
      "sum_diversity_with_female         5  \n",
      "other_geo                            \n",
      "0                          0.017167  \n",
      "1                          0.000000  \n",
      "\n",
      "Chi-Square Statistic for other_geo vs sum_diversity_with_female: 1.8728026705599279\n",
      "P-Value: 0.8664503951953526\n",
      "Degrees of Freedom: 5\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for org_size vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for degree_ordinal vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for case_manager vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for director vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for executive_member vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for direct_service vs sum_diversity_with_female: 0.5721119763197873\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_behavior_analysis vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_business vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_psychology vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_education vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_other_allied vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for midwest_us vs sum_diversity_with_female: 0.29742251413420817\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for south_us vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for west_us vs sum_diversity_with_female: 0.0593627337097935\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for northeast_us vs sum_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for other_geo vs sum_diversity_with_female: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Target column to compare against other categorical variables with chi-squared analyses\n",
    "target_col = 'sum_diversity_with_female'\n",
    "\n",
    "# List of other columns to compare\n",
    "columns_to_compare = ['org_size','degree_ordinal', 'case_manager', 'director', 'executive_member', \\\n",
    "                     'direct_service', 'ceo_behavior_analysis', 'ceo_business', 'ceo_psychology', \\\n",
    "                     'ceo_education', 'ceo_other_allied', 'midwest_us', 'south_us', 'west_us', 'northeast_us', \\\n",
    "                     'other_geo']\n",
    "\n",
    "# Store p-values for Bonferroni correction later\n",
    "p_values = []\n",
    "\n",
    "# Loop through columns and perform chi-square tests\n",
    "for col in columns_to_compare:\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(merged_div_df[col], merged_div_df[target_col])\n",
    "    \n",
    "    # Normalize the proportions row-wise\n",
    "    row_normalized = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    \n",
    "    print(f\"\\nContingency Table for {col} vs {target_col}:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    print(f\"\\nRow-normalized proportions for {col} vs {target_col}:\")\n",
    "    print(row_normalized)\n",
    "\n",
    "    # Run the Chi-Square test using statsmodels\n",
    "    chi2_result = sm.stats.Table(contingency_table).test_nominal_association()\n",
    "\n",
    "    # Output the test results\n",
    "    print(f\"\\nChi-Square Statistic for {col} vs {target_col}: {chi2_result.statistic}\")\n",
    "    print(f\"P-Value: {chi2_result.pvalue}\")\n",
    "    print(f\"Degrees of Freedom: {chi2_result.df}\")\n",
    "\n",
    "    # Store p-values for Bonferroni correction\n",
    "    p_values.append(chi2_result.pvalue)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "adjusted_p_values = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Output the adjusted p-values\n",
    "for col, adj_p in zip(columns_to_compare, adjusted_p_values):\n",
    "    print(f\"\\nAdjusted P-Value (Bonferroni) for {col} vs {target_col}: {adj_p}\")\n",
    "\n",
    "#####??????????????????????????????????????????????????????? keep (here or somewhere else???)\n",
    "#direct_service vs sum_diversity_with_female - but adjusted p-value is .57\n",
    "#midwest_us vs sum_diversity_with_female - but adjusted p-value is .297\n",
    "#west_us vs sum_diversity_with_female - but adjusted p-value is .059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7ed77796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for org_size vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1   2\n",
      "org_size                              \n",
      "1                            1   6  20\n",
      "2                            1   9  70\n",
      "3                            1  15  37\n",
      "4                            0   8  28\n",
      "5                            0   1  18\n",
      "6                            2   4  12\n",
      "7                            0   3   4\n",
      "8                            0   0   5\n",
      "\n",
      "Row-normalized proportions for org_size vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "org_size                                                 \n",
      "1                            0.037037  0.222222  0.740741\n",
      "2                            0.012500  0.112500  0.875000\n",
      "3                            0.018868  0.283019  0.698113\n",
      "4                            0.000000  0.222222  0.777778\n",
      "5                            0.000000  0.052632  0.947368\n",
      "6                            0.111111  0.222222  0.666667\n",
      "7                            0.000000  0.428571  0.571429\n",
      "8                            0.000000  0.000000  1.000000\n",
      "\n",
      "Chi-Square Statistic for org_size vs three_diversity_with_female: 18.989343050063596\n",
      "P-Value: 0.1653568504308084\n",
      "Degrees of Freedom: 14\n",
      "\n",
      "Contingency Table for degree_ordinal vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "degree_ordinal                         \n",
      "-1                           0   1    8\n",
      " 0                           0   0    1\n",
      " 1                           0   5    9\n",
      " 2                           4  30  135\n",
      " 3                           1  10   41\n",
      "\n",
      "Row-normalized proportions for degree_ordinal vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "degree_ordinal                                           \n",
      "-1                           0.000000  0.111111  0.888889\n",
      " 0                           0.000000  0.000000  1.000000\n",
      " 1                           0.000000  0.357143  0.642857\n",
      " 2                           0.023669  0.177515  0.798817\n",
      " 3                           0.019231  0.192308  0.788462\n",
      "\n",
      "Chi-Square Statistic for degree_ordinal vs three_diversity_with_female: 7.404414990812093\n",
      "P-Value: 0.49369259170076485\n",
      "Degrees of Freedom: 8\n",
      "\n",
      "Contingency Table for case_manager vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "case_manager                           \n",
      "0                            4  31  127\n",
      "1                            1  15   67\n",
      "\n",
      "Row-normalized proportions for case_manager vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "case_manager                                             \n",
      "0                            0.024691  0.191358  0.783951\n",
      "1                            0.012048  0.180723  0.807229\n",
      "\n",
      "Chi-Square Statistic for case_manager vs three_diversity_with_female: 0.5004862653295914\n",
      "P-Value: 0.7786114541786234\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for director vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "director                               \n",
      "0                            1  25  114\n",
      "1                            4  21   80\n",
      "\n",
      "Row-normalized proportions for director vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "director                                                 \n",
      "0                            0.007143  0.178571  0.814286\n",
      "1                            0.038095  0.200000  0.761905\n",
      "\n",
      "Chi-Square Statistic for director vs three_diversity_with_female: 3.1713095771701787\n",
      "P-Value: 0.20481363956722487\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for executive_member vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "executive_member                       \n",
      "0                            3  36  147\n",
      "1                            2  10   47\n",
      "\n",
      "Row-normalized proportions for executive_member vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "executive_member                                         \n",
      "0                            0.016129  0.193548  0.790323\n",
      "1                            0.033898  0.169492  0.796610\n",
      "\n",
      "Chi-Square Statistic for executive_member vs three_diversity_with_female: 0.8333034146068898\n",
      "P-Value: 0.659250492094245\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for direct_service vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "direct_service                         \n",
      "0                            4  42  179\n",
      "1                            1   4   15\n",
      "\n",
      "Row-normalized proportions for direct_service vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "direct_service                                           \n",
      "0                            0.017778  0.186667  0.795556\n",
      "1                            0.050000  0.200000  0.750000\n",
      "\n",
      "Chi-Square Statistic for direct_service vs three_diversity_with_female: 0.9999743513123166\n",
      "P-Value: 0.6065384381202404\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_behavior_analysis vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "ceo_behavior_analysis                  \n",
      "0                            3  24   88\n",
      "1                            2  22  106\n",
      "\n",
      "Row-normalized proportions for ceo_behavior_analysis vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "ceo_behavior_analysis                                    \n",
      "0                            0.026087  0.208696  0.765217\n",
      "1                            0.015385  0.169231  0.815385\n",
      "\n",
      "Chi-Square Statistic for ceo_behavior_analysis vs three_diversity_with_female: 1.0426003906642347\n",
      "P-Value: 0.5937480574319967\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_business vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "ceo_business                           \n",
      "0                            2  26  136\n",
      "1                            3  20   58\n",
      "\n",
      "Row-normalized proportions for ceo_business vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "ceo_business                                             \n",
      "0                            0.012195  0.158537  0.829268\n",
      "1                            0.037037  0.246914  0.716049\n",
      "\n",
      "Chi-Square Statistic for ceo_business vs three_diversity_with_female: 4.772839357707783\n",
      "P-Value: 0.09195833556051403\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_psychology vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "ceo_psychology                         \n",
      "0                            4  40  155\n",
      "1                            1   6   39\n",
      "\n",
      "Row-normalized proportions for ceo_psychology vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "ceo_psychology                                           \n",
      "0                            0.020101  0.201005  0.778894\n",
      "1                            0.021739  0.130435  0.847826\n",
      "\n",
      "Chi-Square Statistic for ceo_psychology vs three_diversity_with_female: 1.2201729566508113\n",
      "P-Value: 0.5433038830329009\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_education vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "ceo_education                          \n",
      "0                            3  40  160\n",
      "1                            2   6   34\n",
      "\n",
      "Row-normalized proportions for ceo_education vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "ceo_education                                            \n",
      "0                            0.014778  0.197044  0.788177\n",
      "1                            0.047619  0.142857  0.809524\n",
      "\n",
      "Chi-Square Statistic for ceo_education vs three_diversity_with_female: 2.4033344152665936\n",
      "P-Value: 0.30069247698736323\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for ceo_other_allied vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "ceo_other_allied                       \n",
      "0                            4  41  177\n",
      "1                            1   5   17\n",
      "\n",
      "Row-normalized proportions for ceo_other_allied vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "ceo_other_allied                                         \n",
      "0                            0.018018  0.184685  0.797297\n",
      "1                            0.043478  0.217391  0.739130\n",
      "\n",
      "Chi-Square Statistic for ceo_other_allied vs three_diversity_with_female: 0.8697548326882028\n",
      "P-Value: 0.6473440160106009\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for midwest_us vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "midwest_us                             \n",
      "0                            1  29  151\n",
      "1                            4  17   43\n",
      "\n",
      "Row-normalized proportions for midwest_us vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "midwest_us                                               \n",
      "0                            0.005525  0.160221  0.834254\n",
      "1                            0.062500  0.265625  0.671875\n",
      "\n",
      "Chi-Square Statistic for midwest_us vs three_diversity_with_female: 11.892915249956046\n",
      "P-Value: 0.0026150877516863957\n",
      "Degrees of Freedom: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Table for south_us vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "south_us                               \n",
      "0                            3  28  118\n",
      "1                            2  18   76\n",
      "\n",
      "Row-normalized proportions for south_us vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "south_us                                                 \n",
      "0                            0.020134  0.187919  0.791946\n",
      "1                            0.020833  0.187500  0.791667\n",
      "\n",
      "Chi-Square Statistic for south_us vs three_diversity_with_female: 0.001458688683057615\n",
      "P-Value: 0.9992709215654056\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for west_us vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "west_us                                \n",
      "0                            2  37  143\n",
      "1                            3   9   51\n",
      "\n",
      "Row-normalized proportions for west_us vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "west_us                                                  \n",
      "0                            0.010989  0.203297  0.785714\n",
      "1                            0.047619  0.142857  0.809524\n",
      "\n",
      "Chi-Square Statistic for west_us vs three_diversity_with_female: 4.020963348619108\n",
      "P-Value: 0.13392415130731616\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for northeast_us vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "northeast_us                           \n",
      "0                            5  36  140\n",
      "1                            0  10   54\n",
      "\n",
      "Row-normalized proportions for northeast_us vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "northeast_us                                             \n",
      "0                            0.027624  0.198895  0.773481\n",
      "1                            0.000000  0.156250  0.843750\n",
      "\n",
      "Chi-Square Statistic for northeast_us vs three_diversity_with_female: 1.570794704474962\n",
      "P-Value: 0.4559384976049031\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Contingency Table for other_geo vs three_diversity_with_female:\n",
      "three_diversity_with_female  0   1    2\n",
      "other_geo                              \n",
      "0                            5  45  183\n",
      "1                            0   1   11\n",
      "\n",
      "Row-normalized proportions for other_geo vs three_diversity_with_female:\n",
      "three_diversity_with_female         0         1         2\n",
      "other_geo                                                \n",
      "0                            0.021459  0.193133  0.785408\n",
      "1                            0.000000  0.083333  0.916667\n",
      "\n",
      "Chi-Square Statistic for other_geo vs three_diversity_with_female: 1.1267443935899155\n",
      "P-Value: 0.569286078639036\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for org_size vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for degree_ordinal vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for case_manager vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for director vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for executive_member vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for direct_service vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_behavior_analysis vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_business vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_psychology vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_education vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for ceo_other_allied vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for midwest_us vs three_diversity_with_female: 0.04184140402698233\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for south_us vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for west_us vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for northeast_us vs three_diversity_with_female: 1.0\n",
      "\n",
      "Adjusted P-Value (Bonferroni) for other_geo vs three_diversity_with_female: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Target column to compare against other categorical variables with chi-squared analyses\n",
    "target_col = 'three_diversity_with_female'\n",
    "\n",
    "# List of other columns to compare \n",
    "columns_to_compare = ['org_size','degree_ordinal', 'case_manager', 'director', 'executive_member', \\\n",
    "                     'direct_service', 'ceo_behavior_analysis', 'ceo_business', 'ceo_psychology', \\\n",
    "                     'ceo_education', 'ceo_other_allied', 'midwest_us', 'south_us', 'west_us', 'northeast_us', \\\n",
    "                     'other_geo']\n",
    "\n",
    "# Store p-values for Bonferroni correction later\n",
    "p_values = []\n",
    "\n",
    "# Loop through columns and perform chi-square tests\n",
    "for col in columns_to_compare:\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(merged_div_df[col], merged_div_df[target_col])\n",
    "    \n",
    "    # Normalize the proportions row-wise\n",
    "    row_normalized = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    \n",
    "    print(f\"\\nContingency Table for {col} vs {target_col}:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    print(f\"\\nRow-normalized proportions for {col} vs {target_col}:\")\n",
    "    print(row_normalized)\n",
    "\n",
    "    # Run the Chi-Square test using statsmodels\n",
    "    chi2_result = sm.stats.Table(contingency_table).test_nominal_association()\n",
    "\n",
    "    # Output the test results\n",
    "    print(f\"\\nChi-Square Statistic for {col} vs {target_col}: {chi2_result.statistic}\")\n",
    "    print(f\"P-Value: {chi2_result.pvalue}\")\n",
    "    print(f\"Degrees of Freedom: {chi2_result.df}\")\n",
    "\n",
    "    # Store p-values for Bonferroni correction\n",
    "    p_values.append(chi2_result.pvalue)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "adjusted_p_values = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Output the adjusted p-values\n",
    "for col, adj_p in zip(columns_to_compare, adjusted_p_values):\n",
    "    print(f\"\\nAdjusted P-Value (Bonferroni) for {col} vs {target_col}: {adj_p}\")\n",
    "\n",
    "###########????????????????????????????????????????????? keep (here or somewhere else???)\n",
    "#midwest_us vs three_diversity_with_female - the adjusted p-value is .042!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "916f638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a modeling df that has the possible IVs (minus the multicollinear ones) and individual minority columns\n",
    "# Dropping the columns flagged for multicollinearity\n",
    "initial_modeling_df = merged_div_df.drop(['ed_behavior_analysis', 'community', 'lba', 'position_duration', \\\n",
    "                                         'org_size', 'other_rbt_bcaba'], axis=1)\n",
    "\n",
    "#Dropping the other diversity aggregate columns\n",
    "modeling_df_minority = initial_modeling_df.drop(['binary_diversity', 'binary_diversity_with_female', \\\n",
    "                                                'sum_diversity', 'sum_diversity_with_female', 'three_diversity', \\\n",
    "                                                'three_diversity_with_female'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5d03c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df below that has the initial multicollinearity columns dropped and also the minority columns dropped.\n",
    "#This dataframe will be used to create separate modeling dataframes for each aggregate diversity column\n",
    "aggregate_modeling_df = initial_modeling_df.drop(['race_minority', 'other_gender_minority', 'sex_minority', \\\n",
    "                                                  'religion_minority', 'vet_minority', 'dis_minority', \\\n",
    "                                                  'age_minority'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1b58c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a modeling dataframe with all possible IVs and the binary diversity column \n",
    "#Dropping all other aggregate diversity columns in the aggregate modeling df\n",
    "modeling_df_binary = aggregate_modeling_df.drop(['binary_diversity_with_female', 'sum_diversity', \\\n",
    "                                                'sum_diversity_with_female', 'three_diversity', \\\n",
    "                                                'three_diversity_with_female'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ea3a9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a modeling dataframe with all possible IVs and the three_diversity column \n",
    "#Dropping all other aggregate diversity columns in the aggregate modeling df\n",
    "modeling_df_three = aggregate_modeling_df.drop(['binary_diversity', 'binary_diversity_with_female', 'sum_diversity', \\\n",
    "                                                'sum_diversity_with_female', 'three_diversity_with_female'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ebd84eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a modeling dataframe with all possible IVs and the sum_diversity column \n",
    "#Dropping all other aggregate diversity columns in the aggregate modeling df\n",
    "modeling_df_sum = aggregate_modeling_df.drop(['binary_diversity', 'binary_diversity_with_female', 'three_diversity', \\\n",
    "                                                'sum_diversity_with_female', 'three_diversity_with_female'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4929e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a modeling dataframe with all possible IVs and the binary_with_female column \n",
    "#Dropping all other aggregate diversity columns in the aggregate modeling df\n",
    "modeling_df_binary_with_female = aggregate_modeling_df.drop(['female', 'binary_diversity', 'sum_diversity', \\\n",
    "                                                             'three_diversity', 'sum_diversity_with_female', \\\n",
    "                                                             'three_diversity_with_female'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "77ec80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a modeling dataframe with all possible IVs and the three_with_female column \n",
    "#Dropping all other aggregate diversity columns in the aggregate modeling df\n",
    "modeling_df_three_with_female = aggregate_modeling_df.drop(['female', 'binary_diversity', 'sum_diversity', \\\n",
    "                                                             'three_diversity', 'sum_diversity_with_female', \\\n",
    "                                                             'binary_diversity_with_female'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "71238407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a modeling dataframe with all possible IVs and the sum_with_female column \n",
    "#Dropping all other aggregate diversity columns in the aggregate modeling df\n",
    "modeling_df_sum_with_female = aggregate_modeling_df.drop(['female', 'binary_diversity', 'sum_diversity', \\\n",
    "                                                             'three_diversity', 'three_diversity_with_female', \\\n",
    "                                                             'binary_diversity_with_female'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "06758126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the modeling dataframes as csv files\n",
    "\n",
    "modeling_df_sum_with_female.to_csv('modeling_df_sum_with_female.csv', index=False)\n",
    "\n",
    "modeling_df_three_with_female.to_csv('modeling_df_three_with_female.csv', index=False)\n",
    "\n",
    "modeling_df_binary_with_female.to_csv('modeling_df_binary_with_female.csv', index=False)\n",
    "\n",
    "modeling_df_sum.to_csv('modeling_df_sum.csv', index=False)\n",
    "\n",
    "modeling_df_three.to_csv('modeling_df_three.csv', index=False)\n",
    "\n",
    "modeling_df_binary.to_csv('modeling_df_binary.csv', index=False)\n",
    "\n",
    "modeling_df_minority.to_csv('modeling_df_minority.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ae8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The cells below have some code for modeling. I haven't tried these recently with the newer modeling_df above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80e5f6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.46938775510204084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.17      0.18         6\n",
      "           1       0.60      0.58      0.59        26\n",
      "           2       0.37      0.41      0.39        17\n",
      "\n",
      "    accuracy                           0.47        49\n",
      "   macro avg       0.39      0.39      0.39        49\n",
      "weighted avg       0.47      0.47      0.47        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trying a classification model with the three_diversity column as the target. \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Assuming `df` is your dataframe and 'target' is the column you're predicting\n",
    "X = modeling_df.drop('three_diversity', axis=1)  # Features\n",
    "y = modeling_df['three_diversity']               # Target variable\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "faa4e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        three_diversity   R-squared:                       0.233\n",
      "Model:                            OLS   Adj. R-squared:                  0.036\n",
      "Method:                 Least Squares   F-statistic:                     1.181\n",
      "Date:                Wed, 23 Oct 2024   Prob (F-statistic):              0.213\n",
      "Time:                        10:54:06   Log-Likelihood:                -229.48\n",
      "No. Observations:                 245   AIC:                             561.0\n",
      "Df Residuals:                     194   BIC:                             739.5\n",
      "Df Model:                          50                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================================\n",
      "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "const                                      1.3356      0.339      3.941      0.000       0.667       2.004\n",
      "ceo_behavior_analysis                     -0.2937      0.136     -2.152      0.033      -0.563      -0.025\n",
      "ceo_business                              -0.3084      0.138     -2.229      0.027      -0.581      -0.036\n",
      "ceo_education                             -0.1541      0.143     -1.079      0.282      -0.436       0.127\n",
      "ceo_finance                                0.2434      0.598      0.407      0.685      -0.937       1.424\n",
      "ceo_psychology                             0.0076      0.135      0.056      0.955      -0.258       0.274\n",
      "ceo_human_resources                        0.2595      0.400      0.649      0.517      -0.530       1.049\n",
      "ceo_organizational_behavior_management     0.4116      0.242      1.704      0.090      -0.065       0.888\n",
      "ceo_other_allied                          -0.2338      0.201     -1.165      0.246      -0.630       0.162\n",
      "ceo_other_other                           -0.3550      0.252     -1.410      0.160      -0.852       0.142\n",
      "ceo_unknown                               -0.5836      0.571     -1.023      0.308      -1.709       0.542\n",
      "clinic_outpatient                          0.0666      0.119      0.562      0.575      -0.167       0.300\n",
      "home                                       0.1439      0.116      1.245      0.215      -0.084       0.372\n",
      "hospital_inpatient                         0.1268      0.508      0.250      0.803      -0.875       1.128\n",
      "residential_group_home                     0.0870      0.171      0.508      0.612      -0.251       0.425\n",
      "school                                    -0.0282      0.119     -0.237      0.813      -0.262       0.206\n",
      "vocational_program                        -0.1389      0.199     -0.699      0.485      -0.531       0.253\n",
      "other_setting                              0.4713      0.250      1.883      0.061      -0.022       0.965\n",
      "setting_unknown                            0.5949      0.360      1.651      0.100      -0.116       1.306\n",
      "midwest_us                                -0.2563      0.118     -2.172      0.031      -0.489      -0.024\n",
      "south_us                                   0.1560      0.109      1.433      0.153      -0.059       0.371\n",
      "west_us                                    0.2150      0.121      1.779      0.077      -0.023       0.453\n",
      "northeast_us                               0.1687      0.127      1.327      0.186      -0.082       0.419\n",
      "other_geo                                  0.1763      0.242      0.730      0.466      -0.300       0.653\n",
      "geo_unknown                                0.5949      0.360      1.651      0.100      -0.116       1.306\n",
      "board_member                               0.3201      0.401      0.799      0.425      -0.470       1.110\n",
      "case_manager                               0.0765      0.132      0.580      0.563      -0.184       0.337\n",
      "director                                  -0.0815      0.138     -0.590      0.556      -0.354       0.191\n",
      "direct_service                            -0.0161      0.205     -0.079      0.937      -0.419       0.387\n",
      "executive_member                           0.1102      0.164      0.671      0.503      -0.214       0.434\n",
      "position_other                             0.1660      0.242      0.685      0.494      -0.312       0.644\n",
      "position_unknown                          -0.0926      0.362     -0.256      0.798      -0.806       0.621\n",
      "position_duration                          0.0325      0.056      0.577      0.564      -0.078       0.143\n",
      "degree_ordinal                            -0.0213      0.073     -0.290      0.772      -0.166       0.124\n",
      "ed_business                               -0.0657      0.252     -0.261      0.795      -0.563       0.431\n",
      "ed_education                              -0.0021      0.111     -0.019      0.985      -0.221       0.217\n",
      "ed_finance                                -0.0687      0.194     -0.354      0.724      -0.451       0.314\n",
      "ed_hr                                      0.4576      0.353      1.296      0.197      -0.239       1.154\n",
      "ed_obm                                     0.0986      0.130      0.758      0.449      -0.158       0.355\n",
      "ed_psychology                              0.0140      0.110      0.127      0.899      -0.203       0.231\n",
      "ed_social_work                            -0.0658      0.289     -0.228      0.820      -0.635       0.504\n",
      "ed_domain_other                            0.0989      0.222      0.445      0.657      -0.340       0.537\n",
      "ed_domain_unknown                         -0.0687      0.194     -0.354      0.724      -0.451       0.314\n",
      "bcba                                       0.0476      0.210      0.227      0.821      -0.366       0.462\n",
      "cpa                                        0.2743      0.830      0.330      0.741      -1.363       1.911\n",
      "slp                                        0.8410      0.555      1.515      0.132      -0.254       1.936\n",
      "licensed_psychologist                     -0.1073      0.192     -0.558      0.577      -0.487       0.272\n",
      "licensed_sw                                0.1188      0.606      0.196      0.845      -1.076       1.314\n",
      "other_cert_teaching                        0.1887      0.215      0.877      0.382      -0.236       0.613\n",
      "other_cert                                 0.3173      0.213      1.487      0.139      -0.104       0.738\n",
      "no_cert                                    0.7126      0.751      0.949      0.344      -0.769       2.194\n",
      "cert_unknown                              -0.1281      0.338     -0.379      0.705      -0.794       0.538\n",
      "female                                    -0.1981      0.146     -1.360      0.175      -0.485       0.089\n",
      "==============================================================================\n",
      "Omnibus:                        5.575   Durbin-Watson:                   2.014\n",
      "Prob(Omnibus):                  0.062   Jarque-Bera (JB):                4.074\n",
      "Skew:                          -0.185   Prob(JB):                        0.130\n",
      "Kurtosis:                       2.488   Cond. No.                     1.06e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.25e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming your DataFrame is called 'df' and your target variable is 'y'\n",
    "# 'X' would be your predictor variables\n",
    "X = modeling_df.drop('three_diversity', axis=1)  # predictors\n",
    "y = modeling_df['three_diversity']  # target variable\n",
    "\n",
    "# Add a constant (intercept) to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I haven't tried this out yet. \n",
    "#Since it isn't a specific category per se, you could also treat it like a multi-class classification problem and use multi-class logistic regression and random forest classifer (or others), too! https://scikit-learn.org/1.5/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier\n",
    "\n",
    "#Here's some code if it's useful!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, log_loss\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.evaluate import mcnemar\n",
    "from statsmodels.tools import add_constant\n",
    "\n",
    "# Sample dataset (replace with your own)\n",
    "# Assume a dataset with ordinal target 0, 1, 2\n",
    "\n",
    "X, y = your_data, your_target  # Replace with your dataset\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Ordinal Logistic Regression (Proportional Odds Model)\n",
    "model_ordinal_logit = OrderedModel(y_train, add_constant(X_train), distr='logit')\n",
    "res_logit = model_ordinal_logit.fit(method='bfgs')\n",
    "y_pred_logit = res_logit.predict(add_constant(X_test))\n",
    "\n",
    "# Ordinal Probit Regression\n",
    "model_ordinal_probit = OrderedModel(y_train, add_constant(X_train), distr='probit')\n",
    "res_probit = model_ordinal_probit.fit(method='bfgs')\n",
    "y_pred_probit = res_probit.predict(add_constant(X_test))\n",
    "\n",
    "# Multiclass Logistic Regression (one-vs-rest)\n",
    "clf_logistic_multi = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
    "clf_logistic_multi.fit(X_train, y_train)\n",
    "y_pred_logistic_multi = clf_logistic_multi.predict(X_test)\n",
    "\n",
    "# Random Forest Classifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "acc_logit = accuracy_score(y_test, np.argmax(y_pred_logit, axis=1))\n",
    "acc_probit = accuracy_score(y_test, np.argmax(y_pred_probit, axis=1))\n",
    "acc_logistic_multi = accuracy_score(y_test, y_pred_logistic_multi)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Accuracy (Ordinal Logistic): {acc_logit}\")\n",
    "print(f\"Accuracy (Ordinal Probit): {acc_probit}\")\n",
    "print(f\"Accuracy (Multiclass Logistic): {acc_logistic_multi}\")\n",
    "print(f\"Accuracy (Random Forest): {acc_rf}\")\n",
    "\n",
    "# Log-Loss Calculation\n",
    "logloss_logit = log_loss(y_test, y_pred_logit)\n",
    "logloss_probit = log_loss(y_test, y_pred_probit)\n",
    "logloss_logistic_multi = log_loss(y_test, clf_logistic_multi.predict_proba(X_test))\n",
    "logloss_rf = log_loss(y_test, clf_rf.predict_proba(X_test))\n",
    "print(f\"Log-Loss (Ordinal Logistic): {logloss_logit}\")\n",
    "print(f\"Log-Loss (Ordinal Probit): {logloss_probit}\")\n",
    "print(f\"Log-Loss (Multiclass Logistic): {logloss_logistic_multi}\")\n",
    "print(f\"Log-Loss (Random Forest): {logloss_rf}\")\n",
    "\n",
    "# AIC and BIC for Ordinal Models\n",
    "# For logit and probit models, AIC and BIC can be obtained from the statsmodels fit results\n",
    "print(f\"AIC (Ordinal Logistic): {res_logit.aic}\")\n",
    "print(f\"BIC (Ordinal Logistic): {res_logit.bic}\")\n",
    "print(f\"AIC (Ordinal Probit): {res_probit.aic}\")\n",
    "print(f\"BIC (Ordinal Probit): {res_probit.bic}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_logit = confusion_matrix(y_test, np.argmax(y_pred_logit, axis=1))\n",
    "conf_probit = confusion_matrix(y_test, np.argmax(y_pred_probit, axis=1))\n",
    "conf_logistic_multi = confusion_matrix(y_test, y_pred_logistic_multi)\n",
    "conf_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# McNemar's Test (between two models, for example: logit vs probit)\n",
    "tb_logit_probit = pd.crosstab(np.argmax(y_pred_logit, axis=1), np.argmax(y_pred_probit, axis=1))\n",
    "tb_logit_logistic = pd.crosstab(np.argmax(y_pred_logit, axis=1), y_pred_logistic_multi)\n",
    "chi2_logit_probit, p_logit_probit = mcnemar(tb_logit_probit.to_numpy())\n",
    "chi2_logit_logistic, p_logit_logistic = mcnemar(tb_logit_logistic.to_numpy())\n",
    "print(f\"McNemar Test (Logit vs Probit): chi2 = {chi2_logit_probit}, p = {p_logit_probit}\")\n",
    "print(f\"McNemar Test (Logit vs Logistic Multiclass): chi2 = {chi2_logit_logistic}, p = {p_logit_logistic}\")\n",
    "\n",
    "# Visualization\n",
    "# Confusion Matrix Plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(conf_logit, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Confusion Matrix - Ordinal Logistic')\n",
    "sns.heatmap(conf_probit, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
    "axes[0,1].set_title('Confusion Matrix - Ordinal Probit')\n",
    "sns.heatmap(conf_logistic_multi, annot=True, fmt='d', cmap='Blues', ax=axes[1,0])\n",
    "axes[1,0].set_title('Confusion Matrix - Logistic Multiclass')\n",
    "sns.heatmap(conf_rf, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])\n",
    "axes[1,1].set_title('Confusion Matrix - Random Forest')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve (if applicable for probabilistic models)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr_logit, tpr_logit, _ = roc_curve(y_test, np.argmax(y_pred_logit, axis=1), pos_label=2)\n",
    "fpr_probit, tpr_probit, _ = roc_curve(y_test, np.argmax(y_pred_probit, axis=1), pos_label=2)\n",
    "fpr_logistic_multi, tpr_logistic_multi, _ = roc_curve(y_test, clf_logistic_multi.predict_proba(X_test)[:, 2], pos_label=2)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, clf_rf.predict_proba(X_test)[:, 2], pos_label=2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_logit, tpr_logit, label=\"Ordinal Logistic\")\n",
    "plt.plot(fpr_probit, tpr_probit, label=\"Ordinal Probit\")\n",
    "plt.plot(fpr_logistic_multi, tpr_logistic_multi, label=\"Logistic Multiclass\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=\"Random Forest\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
